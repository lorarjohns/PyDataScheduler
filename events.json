{"info": {"organizer": "PyData", "sponsor": "NumFocus", "location": "Microsoft Conference Center", "address": "11 Times Square, New York, NY 10036", "timezone": "America/New_York"}, "date": [], "sessions": [{"name": "A Few Good Public Servants: How Great Analysis Inspires Action", "performer": ["Kelly Jin"], "@type": "discussion", "description": "What is it like to work in public service? Hear stories from the halls of the Obama White House to New York City Hall of how data and analytics have transformed government in the past decade. From your hometown Mayor's Office of Data Analytics to data teams across the country, learn how a growing number of public servants are using data to ignite change in public policy and operations.", "summary": "Keynote with Kelly Jin", "level": "Novice", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/62/keynote-kelly-jin/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T09:15:00Z", "end": "2019-11-04T10:05:00Z", "duration": "0:50:00"}, {"name": "Time series for scikit-learn people", "performer": ["Ethan Rosenthal"], "@type": "talk", "description": "Time series forecasting and machine learning are often presented as two entirely separate disciplines in the data science literature. When first learning about these topics, I distinctly recall wondering, \u201cWhere does machine learning end and time series begin?\u201d, and \u201cHow do I use features in a time series model?\u201d. This talk will answer these questions by marrying the concepts of time series and machine learning. I will do so by framing time series in a language familiar to anyone who is comfortable with using scikit-learn.\nThis framing will motivate the introduction of the skits library. skits provides a scikit-learn-compatible API for fitting and forecasting time series models. By building off of scikit-learn, skits allows one to build robust and reproducible time series models that enjoy access to the rest of the scikit-learn ecosystem like cross validation tools, standard scoring functions, etc\u2026\nI will close by showing how skits is being used to generate thousands of forecasts per hour of the number of Citi Bikes at every station in NYC using only modest computational resources.", "summary": "This talk will frame the topic of time series forecasting in the language of machine learning. This framing will be used to introduce the skits library which provides a scikit-learn-compatible API for fitting and forecasting time series models using supervised machine learning. Finally, a real-world deployment of skits involving thousands of forecasts per hour will be demonstrated.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/15/time-series-for-scikit-learn-people/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:05:00Z", "end": "2019-11-04T10:45:00Z", "duration": "0:40:00"}, {"name": "Managing Stakeholders: The Key to Successful Data Science for Business", "performer": ["Lauren Oldja"], "@type": "talk", "description": "Tech is people\nA successful data science project is one that results in implementation, e.g., putting a model into production or using the result to support an organizational change. But implementation is only partially a technical problem. Successful project implementation depends on people. \nThis talk will cover how to set up your project for success from the very beginning, including conducting a multi-level stakeholder analysis, establishing expectations through a well-defined communication cadence, and directly involving key stakeholders in problem definition and scoping by applying design thinking and other collaborative techniques borrowed from management science and social science. \nWe will also cover several use case studies (identities withheld to protect the innocent!), drawn from real-world experience in client-facing analytics, where these techniques have been successfully applied. This talk is useful for practicing data scientists of all levels, whether in academia, on an internal analytics team, or client-facing.", "summary": "A successful data science project is one that results in implementation, e.g., putting a model into production or using the result to support an organizational change. But implementation is only partially a technical problem. Successful implementation depends on people. This talk will cover multi-level stakeholder analysis, and techniques for engagement from project scoping to delivery of results.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/51/managing-stakeholders-the-key-to-a-successful-data-science-project/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:05:00Z", "end": "2019-11-04T10:45:00Z", "duration": "0:40:00"}, {"name": "Bringing mental health data to doctors", "performer": ["Bill Lynch"], "@type": "talk", "description": "At NeuroFlow, a mental health company, we\u2019ve built risk stratification, text analysis, and predictive analytics initiatives to enable our users to find and reach their highest-risk patients as soon as possible. This enables our users to be more efficient, and to deliver more personalized, measurement based care even in an area like mental health where they have limited training. This talk is an overview of the lessons we\u2019ve learned from the process of starting, scaling, and deploying data science projects as well as building actionable insights into our data products for health providers. This talk will cover:\n\nThe impact on overall health and costs.\nHow to sift through health data and serve up the metrics that matter.\nThe process for identifying where to start a data driven project in healthcare.\nSteps for building a data driven culture, the do\u2019s and don\u2019ts.", "summary": "There is an edge to gain from the vast amounts of data in Healthcare.  It\u2019s now possible to continuously monitor patients\u2019 health, but where to begin? What data is useful to a doctor who is not trained in mental health care? How do you make findings actionable? In this session, you\u2019ll hear about lessons learned in starting, scaling, & optimizing data science initiatives at a growing tech company.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/40/bringing-mental-health-data-to-doctors/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:05:00Z", "end": "2019-11-04T10:45:00Z", "duration": "0:40:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/100/unconference/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:05:00Z", "end": "2019-11-04T10:45:00Z", "duration": "0:40:00"}, {"name": "AstroPy sprint", "performer": ["Kelle Cruz"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/83/astropy-sprint/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:05:00Z", "end": "2019-11-04T10:45:00Z", "duration": "0:40:00"}, {"name": "Pandas vs Koalas: The Ultimate Showdown!", "performer": ["Amanda Moran"], "@type": "talk", "description": "Introduction\n\nWhy Big Data Matters?\nReview why big data helps get better results\n\nWhat is Pandas?\n\nHigh Level Review and Demo\n\nWhat is Koalas?\n\nHistory of Koalas\nHow to use Koalas\nDemo\n\nShowdown/Conclusion\n\nWhy Koalas over Pandas?", "summary": "No, we won\u2019t be debating which animal is more adorable but we will be discussing the difference between doing data science locally vs doing data science at scale. Most Data Scientist are aware of working with Python and Pandas but you can only work with small amounts of data. With Koalas, Data Scientist can use all the same API's they are familiar with from Panadas but at scale with PySpark.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/52/pandas-vs-koalas-the-ultimate-showdown/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:55:00Z", "end": "2019-11-04T11:40:00Z", "duration": "0:45:00"}, {"name": "A Crash Course in Applied Linear Algebra", "performer": ["Patrick Landreman"], "@type": "talk", "description": "Chances are good that during your education you were required to take a Mathematics course on Linear Algebra, during which you probably covered topics including null spaces, reduced row echelon forms, independence, and a host of similarly abstract concepts. How much of that material do you remember, much less use on a regular basis? Are your eyes glazing over already?\nIt may amaze you to discover the number of things in your life, from your movie recommendations, to your GPS, to your 401k portfolio, that depend on concepts from Linear Algebra. Linear Algebra provides the theory for many core techniques in Data Science and Statistics, notably linear regression and PCA. You can't even talk about a normal distribution in more than one dimension without introducing matrices!\nThis talk will cover the highlights of Applied Linear Algebra. We'll discuss the impacts of familiar topics like eigenvalues and rank and introduce some likely unfamiliar topics such as low-rank approximations, quadratic forms, and definiteness. Throughout the talk, I'll bring in geometric interpretations of the math to help create a visual sense for what is happening, as well as application examples from different science and engineering disciplines. Each concept will be demonstrated using Python and Numpy, often in shockingly few lines of code. \nMy goal is to leave you with an intuition for matrices and linear systems that will unlock your ability to dive into deeper subjects as you continue in your own growth and exploration.\nPrerequisites: a basic knowledge of vectors, matrices, and matrix multiplication", "summary": "Many people think of Linear Algebra as intimidating, difficult, and great for ending conversations at parties. The truth is that Linear Algebra is extraordinarily useful, often unreasonably so. By studying one equation, y = Ax, you will add an arsenal of tools and intuition to your skillset that can be applied in any technical situation (even nonlinear ones).", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/2/a-crash-course-in-applied-linear-algebra/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:55:00Z", "end": "2019-11-04T11:40:00Z", "duration": "0:45:00"}, {"name": "Working with Maps: Extracting Features for Traffic Crash Insights", "performer": ["Jenny Turner-Trauring"], "@type": "talk", "description": "How do you work with map data? Where do you get maps? What counts as a meaningful feature? How do you combine different maps? These are some of the problems we faced when building Insight Lane, an open source volunteer-run civic good project with Data for Democracy that helps cities and advocacy groups better understand traffic crash risk on their roads.\nInsight Lane can take any city\u2019s geocoded crash data and using OpenStreetMaps and other city-specific data sources, build a machine learning model to predict risk of crashes on road segments. You can see the output of the model for select cities at insightlane.org.\nIn this presentation, I will focus on the feature generation step of our pipeline. I will discuss how we process the city maps from OpenStreetMap, how we split roads into segments, and how we include features from city-specific maps and other data sources in our resulting maps. You will see examples using the osmnx, shapely, and pyproj libraries. I will conclude by discussing some interesting features, limitations in the data cities generally make publicly available, and concerns around equity in looking at this sort of data.", "summary": "Have you ever wanted to work with maps, but didn't know where to start? This talk will focus on feature engineering for a real-world project: pulling maps from OpenStreetMap, creating road segments, and adding features from other data sources. The features are used by Insight Lane, building tools for traffic departments and advocacy groups to better understand traffic crash risk.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/8/working-with-maps-extracting-features-for-traffic-crash-insights/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:55:00Z", "end": "2019-11-04T11:40:00Z", "duration": "0:45:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/101/unconference-2/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:55:00Z", "end": "2019-11-04T11:40:00Z", "duration": "0:45:00"}, {"name": "AstroPy sprint", "performer": ["Kelle Cruz"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/84/astropy-sprint-2/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T10:55:00Z", "end": "2019-11-04T11:40:00Z", "duration": "0:45:00"}, {"name": "Scalable Machine Learning with Dask", "performer": ["Tom Augspurger"], "@type": "talk", "description": "Briefly introduce Dask\nIntroduce compute-bound vs. memory-bound problems  \nSee Distributed Joblib for compute-bound problems\nSee Dask-ML's drop-ins for compute-bound problems (GridSearchCV)\nDask collections & Dask-ML estimators for memory-bound problems\nHyperparameter selection for compute & memory-bound problems with Hyperband\nRecap\n\nhttps://docs.dask.org\nhttps://examples.dask.org", "summary": "Python has a great ecosystem for machine learning, especially on relatively\nsmall datasets processed on a single machine. We'll use Dask to scale libraries\nlike NumPy, pandas, and scikit-learn to larger datasets and larger problems.\nWe'll see that problems can be compute- or memory-bound (or both). We'll see\nstrategies for dealing with  these, using a cluster to parallelize our computation.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/27/scalable-machine-learning-with-dask/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T11:40:00Z", "end": "2019-11-04T12:20:00Z", "duration": "0:40:00"}, {"name": "Small Big Data: using NumPy and Pandas when your data doesn't fit in memory", "performer": ["Itamar Turner-Trauring"], "@type": "talk", "description": "Your data is big enough that loading it into memory crashes your program, but small enough that setting up a Big Data cluster isn't worth the trouble.\nYou're dealing with Small Big Data, and in this talk you'll learn the basic techniques used to process data that doesn't fit in memory.\nFirst, you can just buy\u2014or rent\u2014more RAM.\nSometimes that isn't sufficient or possible, in which case you can also:\n\nCompress your data so it fits in RAM.\nChunk your data processing so you don't have to load all of it at once, and if possible parallelize processing to use multiple CPUs.\nIndex your data so you can quickly load into memory only the subset you actually care about.\n\nYou'll also learn how to apply these techniques to NumPy:\n\nCompress using smaller data types and sparse arrays.\nChunk using Zarr.\nParallelize with Dask.\n\nAs well as Pandas:\n\nCompress using smaller data types.\nRead in chunks.\nParallelize with Dask.\nIndex and quickly load partial subsets using HDF5.", "summary": "Your data is too big to fit in memory\u2014loading it crashes your program\u2014but it's also too small for a complex Big Data cluster. How to process your data simply and quickly?\nIn this talk you'll learn the basic techniques for dealing with Small Big Data: money, compression, batching and parallelization, and indexing. In particular, you'll learn how to apply these techniques to NumPy and Pandas.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/4/small-big-data-using-numpy-and-pandas-when-your-data-doesnt-fit-in-memory/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T11:40:00Z", "end": "2019-11-04T12:20:00Z", "duration": "0:40:00"}, {"name": "To comment or not", "performer": ["Veronica Hanus"], "@type": "talk", "description": "While most of us agree that commenting is part of writing maintainable\ncode, it\u2019s very difficult for someone who has not yet worked in a\ncommunity-reviewed codebase to know what is good practice and not. The\nanswers that come back often conflict each other: Code should be DRY,\nbut well-placed comments save future devs. How can someone find the\ncommenting style that is best for them as they learn, grow, &\ncontribute? My survey of 170 long-time developers, Computer Science\nmajors, bootcamp grads, & hobby programmers confirms some expectations\nand brings others into question. Join me for a data-based chat about\nthe biggest pain points caused by our attitudes toward commenting and\nthe steps we can take to encourage a growth mindset and empower\nprogrammers of all levels.", "summary": "Every programmer has asked themselves \u201chow many comments are too\nmany?\u201d To the newest programmers, comments may seem magical\u2013a way of\ndocumenting without giving instructions to the computer. But\ncommenting engages the same vulnerability as more advanced challenges\n(i.e. pair programming & code review) and is likely to pique the\ninsecurity of many programmers (especially the copy-and-paste or\ntutorial-level programmer)!", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/113/comment-or-not/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T11:40:00Z", "end": "2019-11-04T12:20:00Z", "duration": "0:40:00"}, {"name": "Launching a new warehouse with SimPy at Rent the Runway", "performer": ["Meghan Heintz"], "@type": "talk", "description": "This talk will introduce SimPy and how it can be used to build simulations of complex logistics machines using Rent the Runway and it's recent second warehouse opening as a case study.\nSimPy is an object-oriented process-based discrete-event simulation framework based on standard Python. At its core, it allows us to asynchronously schedule and process events. We will delve into how SimPy processes yield events and consume resources and also how we monitor them. \nThen we'll discuss how we put the simulator to work. Some people think we build simulations to predict the future. However, this is rarely the case. We create simulations to understand a process better, learn more about a hypothesis, or test an intervention. At Rent the Runway, we used SimPy to help us understand how our strategies would affect the opening of our second warehouse like which customers would be served by which warehouse and how would we divide inventory between the two.", "summary": "Opening a warehouse is a long and challenging process, not to mention costly. So if you're going to go through all that work to open and operate a warehouse, you'll want to make sure you can run it as efficiently as possible. Enter SimPy (rhymes with \"Blimpie\"). SimPy is a discrete event simulator software library that allows us to asynchronously schedule and process events.", "level": "Novice", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/3/launching-a-new-warehouse-with-simpy-at-rent-the-runway/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T13:20:00Z", "end": "2019-11-04T14:05:00Z", "duration": "0:45:00"}, {"name": "Cleaning, optimizing and windowing pandas with numba", "performer": ["Diego Torres Quintanilla"], "@type": "talk", "description": "This is an advanced talk, aimed at those interested in the internals of Pandas or using Numba to optimize Python code. It does not assume that the audience is already familiar with the Pandas codebase or window operations. However, it does assume that the audience has basic familiarity with different code optimization options, like Cython and Numba.\nAn ordered outline of the talk is as follows (first level bullets are sections of the talk, while second-level bullets are the main ideas I want to convey in that section).\n\n(2 minutes) Speaker intro, legal disclaimer (required by Two Sigma)\n(5 minutes) Discuss how Pandas has achieved its impressive use case coverage by focusing on commonly used features.\nTrade-off was made: optimization at the cost of flexibility.\n\n\n(5 minutes) Introduce window operations via examples.\nExplain what a rolling average is.\nExplain what rolling variance is (very similar to rolling average).\nExplain a more complex window operation: exponentially-weighted rolling average.\nCan pick-and-choose operations and windows to generate many more, like an exponentially-weighted moving variance.\n\n\n(5 minutes) Explain how all of the aforementioned operations are implemented in Pandas, using Cython.\nEven though windows and aggregations can conceptually be combined, they do not share code.\nShow slides that show Pandas' internal code, to demonstrate how there is little shared code.\nAlthough Pandas code is optimized with Cython, user-defined functions (functions passed to DataFrame.apply) are slow because they written in normal Python.\n\n\n(10 minutes) What if we used Numba instead of Cython in the backend?\nUser-defined functions could be just-in-time (JIT) compiled\nSolution: implement window operations as two components, 'aggregators' and 'kernels'.\n'Aggregators' know how to obtain a specific window, 'kernels' know how to apply a mathematical operation to a window.\nNumba JIT-ted classes for aggregators and kernels.\nCan now mix the 'rolling' aggregator with the 'mean' kernel to implement rolling averages!\nCan JIT a user-defined function to create a kernel.\n\n\n(5 minutes) Benefits\nNumba code is easier to debug\nEasier to maintain: componentized operations consolidate multiple implementations\nExisting test suite and API can be preserved\n(8 minutes) Q&A", "summary": "Pandas has accrued a sizable debt in flexibility and maintainability to deliver excellent performance. This talk will show how Pandas maintainers and Two Sigma are using Numba to pay off some of this debt in one of the gnarliest parts of the code: window operations. If merged to mainstream Pandas, this work will deduplicate code, make it easier to debug and make window operations extensible.", "level": "Experienced", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/55/pandas-house-keeping-and-optimization-with-numba/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T13:20:00Z", "end": "2019-11-04T14:05:00Z", "duration": "0:45:00"}, {"name": "Conda-press, or Reinventing the Wheel", "performer": ["Anthony Scopatz"], "@type": "talk", "description": "Conda-press, or Reinventing the Wheel\nConda-press (https://github.com/regro/conda-press) is a new tool that\nlets you transform conda packages (artifacts) into Python wheels. This talk will:\n\ndiscuss why in the world you would want to do such a terrible thing,\ndemonstrate that you can do such a terrible thing (live!),\ndive-in to how such as terrible thing is done, and\ndefine some safety precautions when doing such a terrible thing on your own.\n\nDiscuss\nBuilding software is hard. Luckily, conda-forge is a huge community (1.5k+)\ndedicated to building software, focused on the PyData stack. Unfortunately,\nsome users still want to be able to pip install packages. Double unfortunately,\ncreating binary wheels across many different platforms is often extremely difficult\nfor any package with a C-extension.\nThe central idea behind conda-press is that if there is already a conda-forge\npackage, all of the hard work has already been done! To provide wheels, we\nshould just be able to massage those artifacts into a more circular shape.\nDemonstrate\nBecause we conda-press is just shuffling bits around, managing metadata,\nand not compiling anything new, it is quite fast! This talk will demo\ncreating and installing wheels for a few different packages. For example,\npackages like numpy, scipy, or uvloop are all good candidates. This talk\nmay also demonstrate generating wheels for more esoteric packages that are\nnot related to Python, such as cmake, R, or even Python itself!\nDive-in\nThis talk will discuss the underlying layout of the wheels that are\ncreated and how these wheels are built to work well with other wheels\ncreated by conda-press.\nThis talk will also explain the underlying architecture of conda-press, and how\ntypical workflows are implemented. Conda-press relies on a number of external,\nplatform-specific command line utilities. Conda-press is largely written in\nthe xonsh language to enable this.\nDefense\nThis talk will also offer guidance against common pitfalls when creating\nwheels with conda-press. This includes the distinction between fat and skinny\nwheels, namespace differences between PyPI and conda-forge, and issues with\nprefix substitutions.", "summary": "Conda-press (https://github.com/regro/conda-press) is a new tool that\nlets you transform conda packages into Python wheels. This talk will:\n\ndiscuss why in the world you would want to do such a terrible thing,\ndemonstrate that you can do such a terrible thing (live!),\ndive-in to how such as terrible thing is done, and\ndefine some safety precautions when doing such a terrible things", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/41/conda-press-or-reinventing-the-wheel/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T13:20:00Z", "end": "2019-11-04T14:05:00Z", "duration": "0:45:00"}, {"name": "1:20pm-2pm: A Github for Data, 2pm-3:25pm: OPEN", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "1:20pm-2:00pm, led by Bill Katz\nHow to Make a GitHub for Data: Provenance and Branched Versions\n2:00pm-3:25pm\nOpen\nIn the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/102/unconference-3/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T13:20:00Z", "end": "2019-11-04T14:05:00Z", "duration": "0:45:00"}, {"name": "Discussion: What\u2019s Missing in Python and Data Online Resources?", "performer": ["Debra Williams Cauley"], "@type": "plenary", "description": "We all use online resources, videos, and books when hacking away at a new problem -- whether that's solving a tricky programming problem, manipulating unmanageable data sets, or testing a new machine learning algorithm. Join this session, led by Debra Williams Cauley from Pearson Publishing, to discuss which python and data topics you would like covered by more comprehensive online and video content.", "summary": "Join a discussion to inform the future of published resources relating to open source scientific computing.", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/97/book-exhibition/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T13:20:00Z", "end": "2019-11-04T14:05:00Z", "duration": "0:45:00"}, {"name": "The physics of deep learning using tensor networks", "performer": ["Marianne Hoogeveen"], "@type": "talk", "description": "Why do convolutional networks work well for images? What happens in a neural network when it 'learns\u2019? What is machine learning, actually? These are the type of questions that we should all be wondering about if we use machine learning, and especially deep neural networks, on a daily basis. The field of deep learning is developing rapidly with new architectures being invented to try to solve ever more challenging problems, and this zoo of neural networks needs a taxonomy.\nOne way to bring order to the chaos is by using a physicist's intuition. Bridges are being built, formalizing the link between well-developed fields in physics and neural networks, which allow us to understand extracting information relevant on the macroscopic scale as both a machine learning problem and a problem that has been known in the physics community for a long time, namely why do natural systems look so different at different length scales?\nThe notion of filtering out noise whilst amplifying relevant macroscopic features has a rich history in many-body physics, where a challenge is to go from a microscopic description of a material to accurately describe the effective behavior at macroscopic scales. This is done via what is called the renormalization group, which helps you identify 'irrelevant\u2019 interactions (interactions that have an impact on the very small scale, but tend to get washed out at the larger scale) and 'relevant\u2019 interactions (those that get amplified at the larger scale). If you call the irrelevant interactions \u2018noise\u2019, and the relevant interactions \u2018signal\u2019, you can see a similarity with filtering out noise from signal (your features) in a machine learning model.\nIt turns out that this is no coincidence: these are similar concepts, and a new understanding can be developed around which neural network architectures are suitable for which kind of data.\n\nInductive bias in machine learning [5 min]\nExamples of common neural networks and what the choice of architecture says about our assumptions about the data\nExample: convolutional nets for images\nMeasure of correlations between regions of input data\n\n\nUniversality in machine learning [5 min]\nWhy does deep learning work so well?\nAside: universality in physics\n\n\nA physicist's description of a neural network [10 min]\nTensor Network examples and mappings to some of the beasts in the DNN zoo\nLinking quantum entanglement to measures of correlation between input regions", "summary": "Tensor networks have been used in Physics to find efficient expressions of many-body quantum systems, describing systems from materials to holographic spacetime. As it happens, one can also use tensor networks in machine learning. Exploring the equivalence between these two affords us intuition on inductive bias, which is the set of assumptions that determine our choice of machine learning model.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/10/the-physics-of-deep-learning-using-tensor-networks/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:05:00Z", "end": "2019-11-04T14:50:00Z", "duration": "0:45:00"}, {"name": "Propensity Score Matching: A Non-experimental Approach to Causal Inference", "performer": ["Michael Johns"], "@type": "talk", "description": "Determining whether a marketing campaign caused an increase in sales, or a change to the landing page caused an increase in clicks, requires creating a counterfactual condition \u2013 a condition that can tell us what would have happened if the campaign or site change was not present. Random assignment, the critical process underlying A/B testing, is the preferred method for constructing valid counterfactuals. Random assignment is not always feasible, however. \nThis talk provides an introduction to propensity score matching, a method for creating a counterfactual condition that can be used to approximate an experiment when random assignment is not an option. Propensity score matching is built on the core data science skills of building predictive models and matching algorithm development. I will discuss when and how to implement this method in a business context, with a focus on best practices that have emerged from research and personal experience. Important limitations and special considerations for model building will also be covered. The material will be illustrated using examples taken from real-world projects.", "summary": "Propensity score matching provides an alternative framework for causal inference when random assignment is not possible. The technique draws on core data science skills of predictive model building and algorithm development. Data scientists who need alternatives to experiments will find this a useful and accessible addition to their methodological toolbox.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/28/propensity-score-matching-a-non-experimental-approach-to-causal-inference/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:05:00Z", "end": "2019-11-04T14:50:00Z", "duration": "0:45:00"}, {"name": "Effective Python and R collaboration", "performer": ["Daniel Rodriguez"], "@type": "talk", "description": "Python and R have become essential tools for data scientists and engineers working in various industries, from web giants such as Google and Facebook to scientific researchers, data journalists and more. There is no week that we don't see a breakthrough in science, machine learning, or an app were one of this languages are involved.\nWhile Python and R independently are changing the world, historically there hasn't been that much collaboration between the two languages, and more important between the two communities, on the contrary for the longest time we have been hearing about a \u201cdata science language war\u201d between Python and R. Recently we have seen new initiatives like Apache Arrow and Ursa labs whose main goal is to make it easier for data scientists working in different programming languages to collaborate. Does that mean that the language war between these two is over? If it is, was there a winner and a loser?\nIn general, the competition between them made both winners since long are the days where there was a library or tool in one language that doesn't have a decent alternative in the other. There are still some differences, like R being used more by statisticians and Python by software engineers but it\u2019s weird to see today a data science team where these two types of people are not present, interact and help each other. If that's not the case in your organization you are missing on some diversity that will benefit your team.\nWe will discuss these topics and see what can we learn about our different communities. We will look at some tools that RStudio has been working for collaboration and deployment of data science assets in both R and Python. We will talk about how to build data science projects that use both languages using reticulate, a new library for communication between R and Python, how to deploy shiny apps that use Python for computation and other exciting examples of what's possible today and in the future between two awesome communities. Finally, we will see the RStudio enterprise stack that we offer to our customers for data science collaboration and the new Python and Jupyter functionality.", "summary": "Python and R are extremely powerful by itself, long are the days where there was a library or tool in one language that doesn't have a decent alternative in the other and new initiatives such as Apache Arrow are shaping a great future of collaboration between multiple languages. We will explore how will the future looks for multilingual data science teams in the open-source and in the enterprise.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/17/effective-python-and-r-collaboration/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:05:00Z", "end": "2019-11-04T14:50:00Z", "duration": "0:45:00"}, {"name": "New and Upcoming", "performer": ["Sean Law", " Deepyaman Datta", " Joseph Kearney", " Michael Skarlinski"], "@type": "plenary", "description": "Primrose\nMichael Skarlinski\nDoes your data science team struggle with operationalizing models, building shared infrastructure or on-boarding new members? This talk for you! We will walk through our solution to these issues with our new open source machine learning framework, Primrose. Primrose has empowered us to go \u201cbeyond-the-models\u201d, facilitate the rapid growth of our team, and increase our project velocities. \nkedro\nDeepyaman Datta\nAs professional data scientists and engineers, it is our responsibility to write reproducible, quality code. We show how anyone can use Kedro to build production-ready pipelines from day one\u2014no experience necessary. \nautoimpute\nJoseph Kearney\nReal-world data is messy and missing, yet most statistical models require it to be clean and complete. Analysts are often well versed in modeling, but few are familiar with handling missingness. This talk teaches data professionals best practices for dealing with missingness and introduces Autoimpute, our Python package that helps users grapple with missing data during statistical analysis.\nSTUMPY\nSean Law\nTraditional time series analysis techniques (i.e., visualization, statistics, ARIMA, machine learning, deep learning) have found success in a variety of time series data mining tasks. This presentation introduces a new Python package called STUMPY that offers a simple and intuitive approach for analyzing and understanding time series data.", "summary": "This session will cover four cutting-edge projects in rapid succession. Find out what is new and upcoming in open source scientific computing!", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/96/up-and-coming/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:05:00Z", "end": "2019-11-04T14:50:00Z", "duration": "0:45:00"}, {"name": "Is Spark still relevant? Multi-node CPU and single-node GPU workloads with Spark, Dask and RAPIDS.", "performer": ["Eric Dill"], "@type": "talk", "description": "This talk compares Dask, Spark, and RAPIDS for data science use cases, particularly in traditional business workloads.\n\nWe evaluate these systems by implementing a typical workflow:\nexploring a reasonably-sized data set. The current plan is the curated Anaconda download statistics (300M rows), though I may go upstream to get a >10B row dataset\ntraining a simple predictive model\nscoring new data with that model\nbuilding a dashboard to surface the results to internal stakeholders\n\nThis talk compares the implementation in each of these ecosystems along the following axes: (a) infrastructure requirements for each, (b) difficulty of development and deployment for someone already familiar with Spark on YARN but not Dask or RAPIDS and (c) performance (core-hours and gigabyte-hours and cost). We source data from s3. For Spark, we use EMR on AWS. For Dask we reuse the same EMR on an AWS cluster along with EKS and bare ec2 instances. For RAPIDS (GPU-based) we use a local workstation and also a GPU-equipped cloud instance. \nThis talk will end with the practicality of Dask, Spark, and RAPIDS. We will address such questions as\n* \u201cWhat if I already have a large Spark / Hadoop investment?\u201d\n* \u201dFor a greenfield project, what\u2019s the recommendation?\u201d\n* \u201cNew architectures in businesses are especially hard to adopt. Are there any really compelling reasons to use Dask / RAPIDS over Spark?\u201d\nThis talk will be especially relevant if you are already familiar with one or more of these tools but will be of general interest to practicing data scientists, data engineers, IT, DevOps, analysts, and engineering or analytics management.", "summary": "We compare Dask, Spark, and RAPIDS in a typical business workload: dataset exploration, model building, prediction and dashboarding / reporting. Of specific interest are infrastructure, ease of use and some general musings on the practicality of these compute frameworks. This talk will specifically address the relevance of these systems to the unique brand of problems we face in modern businesses.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/57/is-spark-still-relevant-multi-node-cpu-and-single-node-gpu-workloads-with-spark-dask-and-rapids/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:50:00Z", "end": "2019-11-04T15:30:00Z", "duration": "0:40:00"}, {"name": "Same API, Different Execution", "performer": ["Saul Shanabrook"], "@type": "talk", "description": "(End of) Moore's Law\nRelationship with von Neuman Architecture\nInfluence on Python and NumPy\nSolutions from Deep Learning Frameworks\nDomain Specific Languages: the general framework\nA Python and community first approach\nmetadsl example\nWe need your help!", "summary": "Can the Python data science and scientific computing ecoystem remain in the hands of community open source projects? Or will increasingly complex performance and hardware requirements leave room only for vertically integrated corporate sponsored projects?", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/50/same-api-different-execution/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:50:00Z", "end": "2019-11-04T15:30:00Z", "duration": "0:40:00"}, {"name": "Using Graph Nets (GNs) to predict molecular properties", "performer": ["Chaya D Stern", " Yuanqing Wang"], "@type": "talk", "description": "Computational models at early stages of drug discovery campaigns have been shown to have the potential to reduce the failure rate, time span, and cost associated with designing a new drug. These models are trained on computational expensive QM data to achieve an appropriate level of accuracy. Here, we explore the possibility of approximating such calculations using faster, but just as accurate methods. We introduce a GN model where both nodes (atoms) and edges (bonds) are attributed, thus the problem could thereby be formed as a multitask regression problem. Operating directly on the topological space of molecules, GNs inherently preserve the permutation invariance and equivariance and allow the maximal latent representation sharing among atoms and bonds. To prove this point, we examine the behaviors of a GN model in cases where it predicts per-atom, per-bond, and per-molecule properties independently, jointly, or predicts one with others given.\nThe package that implements the GN model (github.com/choderalab/gimlet) is written in python with TensorFlow 2.0, and does not have any dependencies apart from it. Reading and writing of molecules, realization of aromaticity, typing, and simply molecular mechanics energy calculations, are all written as tf.functions and thus could be compiled into graphs and paralleled when dealing with large datasets.", "summary": "In this talk we will be showing how Graph Nets (GNs)\u2014a set of statistical models that directly operate on molecular topology by updating and aggregating information between atoms and bonds\u2014can approximate per-atom, per-bond, and per-molecule properties derived by quantum mechanics (QM), with errors within the uncertainty thereof, and an over-500-fold speed up", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/44/using-graph-nets-gns-to-predict-molecular-properties/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:50:00Z", "end": "2019-11-04T15:30:00Z", "duration": "0:40:00"}, {"name": "What's now in NumFOCUS projects? (Part 1)", "performer": ["Ralf Gommers", " Colin Carroll", " Francesc Alted", " Ryan Abernathey", " Tom Augspurger"], "@type": "plenary", "description": "This round of lightning updates will include:\n\nNumPy & SciPy (Ralf Gommers)\npymc3 (Colin Carrol)\nblosc & pytables (Francesc Alted)\nxarray (Ryan Abernathey)\ndask (Tom Augspurger)", "summary": "Quick updates from NumFOCUS projects", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/98/whats-now-numfocus-projects-part-1/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T14:50:00Z", "end": "2019-11-04T15:30:00Z", "duration": "0:40:00"}, {"name": "Quantifying uncertainty in machine learning models", "performer": ["Samuel Rochette"], "@type": "talk", "description": "We'll see why and how it is very important to compute uncertainty in inferential statistics and predictive machine learning models.\n1) Deep dive in random forest\nRandom Forest gives us naturally an estimation of the distribution for each sample thanks to the bagging technique.\n2) Generalisation for regression\nThe quantile loss is useful to compute prediction intervals for every regression model. It is however a computationally costly. Certain loss like cosh can help against this con.\n3) What about classification\nIn classification, probability is a measure of the uncertainty... but does every model give us good probabilities ? Let plot some reliability curve to check if we need to calibrate the output with a sigmoid or an isotonic regression !", "summary": "Many models give a lot more information during the inference process that we usually know. We will begin with an intrinsic estimation of all the distribution with random forest algorithm. Then we will extend those \"prediction intervals\" to almost every regression models thanks to the quantile loss. Eventually we will discuss about probability calibration to measure uncertainty in classification.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/24/quantifying-uncertainty-in-machine-learning-models/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T15:40:00Z", "end": "2019-11-04T16:25:00Z", "duration": "0:45:00"}, {"name": "Generating realistic, differentially private data sets using GANs", "performer": ["Joshua Falk"], "@type": "talk", "description": "Individual-level data is very valuable to researchers, as it allows for arbitrary analyses. However, the value of individual-level data must be balanced against the privacy concerns of individuals who appear in data sets. In this talk, I discuss an approach for generating synthetic data sets that preserve relevant statistical properties of input data while also providing privacy guarantees for individuals who appear in the data. I offer an implementation of a differentially private generative adversarial network (DPWGAN) using PyTorch. This method provides mathematical guarantees of privacy. To illustrate how this method works, I apply the DPWGAN to ACS PUMS data, a collection of individual-level from the U.S. Census Bureau. I show that the DPWGAN models correlations in the data, and that cross tabs on the synthetic data are close to those in the original data. The PyTorch code is available and open source. Participants will come away with an understanding of the basics of differential privacy and generative adversarial networks, as well as the ability to apply the DPWGAN code to their own data sets.", "summary": "The value of individual-level data for research must be balanced against the privacy concerns of individuals. I discuss an approach for generating synthetic data sets that preserve relevant statistical properties of input data while also providing privacy guarantees for individuals in the data.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/1/generating-realistic-differentially-private-data-sets-using-gans/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T15:40:00Z", "end": "2019-11-04T16:25:00Z", "duration": "0:45:00"}, {"name": "Simplified Data Quality Monitoring of Dynamic Longitudinal Data: A Functional Programming Approach", "performer": ["Jacqueline Gutman"], "@type": "talk", "description": "Ensuring the quality of data we deliver to customers or provide as inputs to models is often one of the most underappreciated and yet time-consuming responsibilities of a modern data scientist. This task can be a challenge when working with static data, but when we have access to dynamic, longitudinal, continuously updating data, that complexity becomes an asset. At Flatiron Health, a cancer research company acquired for billions of dollars partly due to the complexity, breadth, and immediacy of its data, we harness the dynamic nature of our ever-changing datasets to define a robust and systematic process for early and actionable detection of data quality concerns.\nOutline\nIntroduction: I am a data scientist and cancer researcher working with electronic medical records in oncology at the healthcare technology company Flatiron Health (3 min)\nProblem: How do we identify issues in data that is longitudinal, demands recency, and is dynamic within and between people over time? (4 min)\n\nHow quickly can we catch data quality issues?\nWhat are the downstream consequences if these issues are missed?\n\nAs a cancer researcher, undetected issues in data quality have real downstream consequences that impact patients' lives--for example, data could be used as evidence providing that one cancer drug works better than another when in fact the opposite is true.\nSolution: Build a continuous and efficient process to monitor data quality, simplify dimensions on which the data has recently changed, and create actionable flags for investigation (1 min)\nConceptual framework: We continually freeze and version our data and we treat each distinct versioned snapshot in time of our data as a self-contained input to readable, declarative functions that summarize key quality metrics of each dataset. We can borrow from 5 key principles and paradigms used in functional programming to reason about data quality in an enjoyable, scalable, and self-documenting process. (5 min)\n5 Key Pillars of Enjoyable, User-Friendly Data Quality Monitoring (with code snippets):\nWe provide code using functools + Pandas to demonstrate how we reason about multiple evolving versions of our data to assess its quality\n\nReadability: Abstraction and Declarative intent (3 min)\nCompositionality: Reasoning with higher level functions (3 min)\nReproducibility: Avoiding side effects and external states (3 min)\nEfficiency: Laziness and caching (3 min)\nRobustness: Testability and elegant error handling (3 min)\n\nWrap Up: We are all responsible for protecting the integrity of the data we work with. This framework allows us to strive towards readibility, compositionality, reproducibility, efficiency, and robustness in monitoring of quality. (2 min)", "summary": "Data scientists are often tasked with being the first to detect issues of data quality that may have serious consequences for downstream consumers of their data. We will demonstrate how to simplify data quality monitoring through a functional programming approach that empowers the 5 key pillars of a user-friendly workflow: readability, compositionality, reproducibility, efficiency, and robustness.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/46/simplified-data-quality-monitoring-of-dynamic-longitudinal-data-a-functional-programming-approach/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T15:40:00Z", "end": "2019-11-04T16:25:00Z", "duration": "0:45:00"}, {"name": "3:40pm-5pm: Role Play annotation facilitator training, 5:10pm-5:50pm: STUMPY & Time-series analysis", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "3:40pm-5:00pm\nRole Playing Annotation Pre-Workshop Facilitator Training\n5:10pm-5:50pm, led by Sean Law\nSTUMPY and Time-Series Analytics\nIn the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/103/unconference-4/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T15:40:00Z", "end": "2019-11-04T16:25:00Z", "duration": "0:45:00"}, {"name": "Panel Discussion: My First Open Source Contribution", "performer": ["Bryan Cross", " Chris Fonnesbeck", " Hannah Aizenman", " Julia Signell", " Steve Dower"], "@type": "plenary", "description": "Are you interested in becoming an open source contributor but not sure where to start? Join us for a panel discussion and Q&A to hear from core language and library contributors and maintainers about their experiences contributing to and reviewing contributions to open source. During the Q&A, you will have the opportunity to ask questions about how to get started, what you can expect, or anything else you\u2019ve been wondering about contributing to open source. To help get you started, at the end of the session you can pick from a curated set of open issues on several open source projects that are good for first-time contributors. Come back to the conference having made some progress on your issue, and we\u2019ll reward you with some swag! The open source community looks forward to your contributions.", "summary": "Join us for a panel discussion and Q&A to hear from core language and library contributors and maintainers about their experiences contributing to and reviewing contributions to open source.\nWith Moderator Bryan Cross.", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/91/panel-discussion-my-first-open-source-contribution/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T15:40:00Z", "end": "2019-11-04T16:25:00Z", "duration": "0:45:00"}, {"name": "Spark Backend for Ibis: Seamless Transition Between Pandas and Spark", "performer": ["Li Jin", " Hyonjee Joo"], "@type": "talk", "description": "Pandas is the de facto standard (single-node) DataFrame implementation in Python. However, as data grows larger, pandas no longer works very well due to performance reasons. On the other hand, Spark has become a very popular choice for analyzing large dataset in the past few years. However, there is an API gap between pandas and Spark, and as a result, when users switch from pandas to Spark, they often need to rewrite their programs.\nIbis is a library designed to bridge the gap between local execution (pandas) and cluster execution (BigQuery, Impala, etc). In this talk, we will introduce a Spark backend for ibis and demonstrate how users can go between pandas and Spark with the same code.", "summary": "Ibis a library that provides unified pandas-like API on top of both single-node local execution (e.g. pandas) and multi-node remote execution (e.g. BigQuery, Impala). In this talk, we will introduce a new backend for executing Ibis programs on Spark and show how you can write analytics that run on both Spark and pandas.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/22/spark-backend-for-ibis-seamless-transition-between-pandas-and-spark/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T16:25:00Z", "end": "2019-11-04T17:10:00Z", "duration": "0:45:00"}, {"name": "Build an AI-powered Pet Detector in Visual Studio Code", "performer": ["Katherine Kampf"], "@type": "talk", "description": "Ever wondered what breed that dog or cat is? Let\u2019s build a pet detector service to recognize them in pictures! In this talk, we will walk through the training, optimizing, and deploying of a deep learning model by using VS Code and the Azure Machine Learning service. We will use transfer learning to recognize dog and cat breeds. Next, we\u2019ll optimize the model using Azure Machine Learning service to improve the model accuracy. Putting on our developer hat, we'll then refactor the notebooks into Python modules using VS Code. Finally, we will deploy the model as a web service in Azure. Come to see how Azure and VS Code makes AI and machine learning development and deployment easy.", "summary": "Ever wondered what breed that dog or cat is? Let\u2019s build a pet detector service to recognize them in pictures!", "level": "Novice", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/75/build-an-ai-powered-pet-detector-in-visual-studio-code/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T16:25:00Z", "end": "2019-11-04T17:10:00Z", "duration": "0:45:00"}, {"name": "Geo Experiments and CausalImpact in Incrementality Testing", "performer": ["Jessica Tyler"], "@type": "talk", "description": "While A/B testing can be a powerful means of comparing the performance of two variants, it can be difficult in practice to ensure that control and treatment groups are being determined in an unbiased manner. An additional layer of complexity is introduced in cases where we would like to find the incremental effect of a treatment: since this requires us to estimate a counterfactual (what would have happened if the treatment had not been applied), a simple comparison between test and control is no longer sufficient.\nGeo experiments and CausalImpact provide a framework to handle two of the main areas of complexity involved in incrementality testing: experimental design and causal inference. Geo experiments randomly assign geographies to control and test groups. CausalImpact, a methodology developed by Google with packages available in Python and R, uses Bayesian structural time series models that not only provide estimates of the treatment's incremental impact on our outcome of interest, but confidence bounds as well. In this talk, I will cover a real example from HelloFresh, where we used these two methods to estimate the incremental CAC (customer acquisition cost) of a YouTube campaign.", "summary": "How can we find the incremental number of conversions driven by a new marketing campaign? This presentation will cover how we used geo experiments and CausalImpact to answer this question at HelloFresh.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/47/geo-experiments-and-causalimpact-in-incrementality-testing/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T16:25:00Z", "end": "2019-11-04T17:10:00Z", "duration": "0:45:00"}, {"name": "Panel Discussion: Pitching Open Source Up Your Management Chain", "performer": ["Gil Forsyth", " David Palaitis", " Kevin Fleming", " Mike McCarty"], "@type": "plenary", "description": "Moderated by Gil Forsyth, a maintainer of the popular xonsh shell. With guest leaders in industry, Kevin Fleming (Member of the CTO office at Bloomberg), Mike McCarty (Director at the Capital One Center for Machine Learning) and David Palaitis (Managing Director at Two Sigma).\nExample questions for our guests include: How do you have to pitch management on presenting at conferences? How do you pitch management on contributing to existing projects?", "summary": "Open source tools are critical to data science across a range of industries. Engaging with the community can be beneficial for everyone, but sometimes you have to convince your management of that! The panel will talk about how they have successfully brought open source into their companies.", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/82/pitching-open-source-your-management-chain/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T16:25:00Z", "end": "2019-11-04T17:10:00Z", "duration": "0:45:00"}, {"name": "What we learned by running a large custom Bayesian forecasting model in production", "performer": ["Jens Fredrik Skogstrom"], "@type": "talk", "description": "At Kolonial.no, Norway\u2019s largest online groceries retailer, sales forecasts are crucial for an efficient and scalable warehouse and distribution operation. To fill this need, we built and implemented our own custom Bayesian forecasting model with hundreds of random variables in PyMC3- and learned a lot in the process. In this talk, we share our thoughts on when to do what we did, and more important, when not to do it. We also share how we stumbled into numerous pitfalls on the way but managed to climb out again.\nThis talk should be interesting for data scientists who consider running their own large Bayesian models, managers of data scientists proposing something like this and engineers interested in how we managed to put this thing into production and how we work to improve it over time.  \nNo specific background knowledge is required to appreciate the main take-aways of this talk, but prior knowledge of Bayesian modelling and PyMC3 will probably make you appreciate the talk even more.", "summary": "At Kolonial.no we built and implemented our own custom Bayesian forecasting model in PyMC3 - and learned a lot in the process. In this talk, we share our thoughts on when to do what we did, and more important, when not to do it. We also share how we stumbled into numerous pitfalls on the way but managed to climb out again.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/39/what-we-learned-by-running-a-large-custom-bayesian-forecasting-model-in-production/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T17:10:00Z", "end": "2019-11-04T17:50:00Z", "duration": "0:40:00"}, {"name": "Semantic modeling of data science code", "performer": ["Evan Patterson"], "@type": "talk", "description": "As suggested by the name of Project Jupyter (\u201cJUlia-PYThon-R\u201d), contemporary data science is increasingly pluralistic, involving several popular programming languages and countless software packages. While a diverse and growing ecosystem is generally a boon to the field, it can impede data scientists from communicating and sharing knowledge effectively, both with each other and with their collaborators in other fields. Building intelligent tools for data scientists and conducting automated meta-analyses are also more difficult.\nWe present our ongoing efforts to automatically construct semantic models of data science code, expressed in terms of general concepts and independent of any specific programming language or library. We explain and demonstrate by example the elements of our process: program analysis tools for Python and R; a fledgling Data Science Ontology; and an ontology-driven algorithm for semantic enrichment, implemented in Julia. We also suggest possible applications and future directions for this technology.\nAttendees will find it helpful to have a working knowledge of at least one programming language commonly used in data science, such as Python or R. All project components are available as open source software under a permissive license.", "summary": "Programming languages and libraries are proliferating in the data science community. In an effort to reduce communication barriers and enable automation and intelligent tooling, we are developing software to automatically construct language-agnostic semantic models of data science code written in Python or R. In this talk, we introduce our methods and illustrate them by example.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/37/semantic-modeling-of-data-science-code/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T17:10:00Z", "end": "2019-11-04T17:50:00Z", "duration": "0:40:00"}, {"name": "Zarr vs. HDF5", "performer": ["Joe Jevnik"], "@type": "talk", "description": "Zarr is a modern library and data format for storing chunked, compressed N-dimensional data.\nHDF5 is a mature (20 years old) library and data format which is also designed to handle chunked compressed N-dimensional data.\nThis talk will attempt to compare these two similar technologies, allowing the audience to pick the correct tool for their problem.\nThis talk will begin by covering basic usage of both zarr and h5py, a Python interface to HDF5.\nZarr and h5py have a nearly identical interface, so in many cases you can swap libraries without requiring any code changes.\nIn this section we will cover some of the core concepts behind working with chunked multidimensional data.\nThe first important difference between zarr and HDF5 that we will cover is their handling of compressors and filters.\nCompressors and filters are how both zarr and HDF5 implement compression and other data transformations like checksumming.\nIn both libraries, compressors and filters are composable units which can form a filter pipeline that transparently acts on a dataset.\nThis talk will cover both libraries' built in filters, as well as how they each support user-defined extensions.\nNext, we will cover the differences in how data is stored in both libraries.\nBoth zarr and HDF5 provide multiple concrete storage types, ranging from a single file on local disk, to distributed files across a cloud object store like Amazon S3.\nThis talk will cover the default and built in formats for both zarr and HDF5.\nThis talk will also cover how zarr and HDF5 can be extended to support new storage formats.\nWhile both zarr and HDF5 can be used with the same data, the choice of filters, storage, and other implementation details can cause large differences in performance.\nThroughout this talk, profiling methods will be shown to help the audience design and run their own experiments to compare zarr and HDF5 with their own data.\nBefore the talk, audience members should:\n\nKnow Python syntax and vocabulary.\nKnow basic numpy usage, including working with multidimensional arrays.\n\nBy the end of this talk, audience members will:\n\nKnow the basic use cases for zarr or HDF5.\nUnderstand the basics of filters and compressors in zarr and HDF5.\nUnderstand the basics of data storage in zarr and HDF5.\nBe familiar with how zarr and HDF5 can be extended.\nBe able to begin comparing zarr and HDF5 with their own data.", "summary": "Zarr and HDF5 are libraries and data formats for storing chunked, compressed N-dimensional data.\nThis talk will give a brief introduction to the their shared functionality, but then focus on the their differences.\nThis talk cover how show how each library handles compression, data storage, and extension points.\nThis talk should help the audience choose the correct tool for their problem.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/42/zarr-vs-hdf5/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T17:10:00Z", "end": "2019-11-04T17:50:00Z", "duration": "0:40:00"}, {"name": "What's now in NumFOCUS projects? (Part 2)", "performer": ["Hannah Aizenman", " Anthony Scopatz", " Bryan Van de Ven", " Thomas J Fan"], "@type": "plenary", "description": "This round of lightning updates will include\n\nbokeh (Bryan Van de Ven)\nMatplotlib (Hannah Aizenman)\nscikit-learn (Thomas Fan)\nconda-forge (Anthony Scoptaz)\nnteract (Safia Abdalla)\njupyter (Ana Ruvalcaba)", "summary": "Quick updates from NumFOCUS projects", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/99/whats-now-numfocus-projects-part-2/", "date": "Monday Nov. 4, 2019", "start": "2019-11-04T17:10:00Z", "end": "2019-11-04T17:50:00Z", "duration": "0:40:00"}, {"name": "Data science at The New York Times:  a mission-driven approach to personalizing the customer journey", "performer": ["Chris Wiggins", " Anne Bauer"], "@type": "discussion", "description": "How does The New York Times use data science to further its mission? We'll talk about the use of machine learning throughout the company, from social media promotion to targeted advertising to content recommendations, and the cross-team collaborations that make it possible.", "summary": "Keynote by Anne Bauer & Chris Wiggins.", "level": "Novice", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/63/keynote-bauer-wiggins/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T09:15:00Z", "end": "2019-11-05T10:05:00Z", "duration": "0:50:00"}, {"name": "Deep Dive into scikit-learn's Hist GradientBoosting Classifier and Regressor", "performer": ["Thomas J Fan"], "@type": "talk", "description": "Gradient boosting decision trees (GBDT) is a powerful machine-learning technique known for its high predictive power with heterogeneous data. In scikit-learn 0.21, we released our own implementation of histogram-based GBDT called HistGradientBoostingClassifier and HistGradientBoostingRegressor. This implementation is based on Microsoft's LightGBM and makes use of OpenMP for parallelization. In this talk, we will:\n\nLearn about the underpinnings of scikit-learn's histogram-based gradient boosting algorithm.\nGain an intuition about HistGradientBoostingClassifier/Regressor's hyper-parameters.\nCompare the performance of scikit-learn's implementation with other GBDT libraries such as XGBoost, CatBoost, and LightGBM.\n\nThis talk is targeted to those familiar with machine learning and want a deeper understanding of scikit-learn's histogram-based gradient boosting trees.", "summary": "Gradient boosting decision trees (GBDT) is a powerful machine-learning technique known for its high predictive power with heterogeneous data. In this talk, we will explore scikit-learn's implementation of histogram-based GBDT called HistGradientBoostingClassifier/Regressor and how it compares to other GBDT libraries such as XGBoost, CatBoost, and LightGBM.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/5/deep-dive-into-scikit-learns-histgradientboostingclassifier-and-regressor/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "Julia for Pythonistas", "performer": ["Kelly Shen"], "@type": "talk", "description": "From numerical method class projects to computational physiology research to improving statistical rigoriness of A/B testing at Etsy, I have used Julia in a few different settings. In this talk, I will share my experience solving a variety of problems using Julia and why I find the language to be accessible and unique. I will show the audience how easy it is to pick up Julia, especially coming from a Python background, and how the two languages are similar and different. I will also walk the audience through some exercises that will help them get started programming in Julia.", "summary": "In this talk, I will introduce the audience to Julia and show you how seamless it is to start programing in Julia from a Python background. I will talk about my experience using Julia in a few different projects and why I\u2019ve found Julia an accessible yet special language with a lot of potential. I will also share what I love about the Julia community.", "level": "Novice", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/30/julia-for-pythonistas/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "Improve the efficiency of your Big Data application", "performer": ["Francesc Alted", " Christian Steiner"], "@type": "talk", "description": "Our libraries feature a novel approach to store and process data compressed in-memory to achieve low-memory consumption while maintaining high-performance.\nCaterva: Built on top of C-Blosc2, implements a simple multidimensional container for compressed data. It adds the capability to store, extract, and transform compressed data in these containers, either in-memory or on-disk.\nIronArray: Built on top of Caterva and C-Blosc2, adds type-safety as well as a computational engine, so that matrix and vector calculations are performed efficiently on top of compressed and multidimensional containers.\nCaterva\nWhile there are several existing solutions for storing compressed data (HDF5 is one of the most well known examples), Caterva brings the following novel features which set it apart from its competitors:\n\n\nIn-Memory compression: By default the multidimensional container is stored entirely in-memory in a compressed form, this allows for low-memory consumption while still providing high-performance to access the data.\n\n\nCompression algorithms: By using C-Blosc2 the user can choose from various state of the art compression algorithms and compression filters, which allow to achieve the optimal trade-off between performance and memory efficiency.\n\n\nMinimize memory copies: Compared to other solutions which often treat compression as an after-thought, Caterva minimizes the amount of memory copies as much as possible and hence, increases performance.\n\n\nOn-disk persistence: Both in-memory and on-disk paradigms are supported the same way. This means that the same API can be used for data that can be either in-memory or on-disk.\n\n\nSupport for a plain buffer data layout.  This allows for essentially zero-copy data sharing among existing libraries (NumPy), so one can use the existing functionality directly in Caterva containers without loosing performance.\n\n\nIronArray\nIronArray implements a computational engine that is optimized to deal with compressed data. IronArray adds type definitions to Caterva containers and takes every measure to reduce the compression overhead to seamlessly perform calculations on these; its ultimate goal is to be able to perform computations on compressed containers at the same speed than by using uncompressed containers.\nDuring our talk, we will introduce Caterva and IronArray features by using cat4py, a Python wrapper for Caterva and IronArray for Python.", "summary": "If you're using NumPy and your data uses too much memory or requires too much computational resources this talk is for you! We'll introduce Caterva and IronArray, two libraries that, when used together, can greatly improve the efficiency and reduce the cost of your big data applications.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/13/improve-the-efficiency-of-your-big-data-application/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "The Echo-Chamber of Your Social Media Feed", "performer": ["Tamar Yastrab"], "@type": "talk", "description": "Companies have invested significant resources into developing advertisement algorithms that analyze users\u2019 data and display relevant media in their feed. People are more and more shocked that the products they searched for now appear on the tabs of their social media and email accounts. Other than proprietary reservations about one\u2019s data, this 3-way relationships is seemingly symbiotic; Ad companies\u2019 services are being shared with costumers most inclined to buy-in, users see more of the product they like, and the company generating these relationships reaps a hefty profit. \nBut how do these algorithms work, and what are the effects of seeing only the information companies like Facebook and Google expect you to like? \nCompanies use machine learning programs that try and identify common phraseology and links as the parameters for classifying similar content. The words you tend to click on will be the ones you tend to see. The more a user clicks on media that have a similar progression of words and ideas, the sharper the computer\u2019s vocabulary becomes, and it will anticipate which new articles and websites the user would herself visit. And while this makes social media browsing all the more pleasant, there is a severe impact on the variety of perspectives a user sees.  \nNews articles are some of the most common media that is filtered by these algorithms. Politicians cater their campaigns to the political perspective assumed by a user\u2019s data. This means that voters are only seeing a very small fraction of larger political conversations and are forming opinions without all the facts. Repeated rhetoric is literally what baits the algorithm, so users will only be suggested articles that have nearly identical viewpoints. This motivates journalists to look for fiery phraseology that will increase their shareability, often at the expense of nuance. As a student, I would like to speak to how this has affected intellectual conversations on university campuses, where students are generally exposed to limited perspectives with much less appreciation for complexity. \nIn conclusion, I am not arguing for the moral correctness or incorrectness of this methodology for displaying media, but rather I want to raise awareness in the data mining community about the larger impacts of data analysis in the policy worlds of economics, sociology, and politics generally. Data will affect how today\u2019s youth form decisions, and serious threats to diverse exposure are posed when feeds are too severely exploited.", "summary": "Data algorithms that anticipate what users want to see create an echo chamber of articles expressing a singular opinion. Journalist are inclined to use strong rhetoric that will be consistently favored by these ML programs and at the expense of complexity. This filtering process has bolstered polarized politics, especially on college campuses where more students turn to social media for news.", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/35/the-echo-chamber-of-your-social-media-feed/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/104/unconference-5/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "Fireside Chat", "performer": ["Chris Wiggins", " Anne Bauer"], "@type": "discussion", "description": "A fireside chat with keynote speakers Anne Bauer and Chris Wiggins.\nAnne is a lead data scientist at The New York Times, where she heads the Algorithmic Recommendations team. Chris Wiggins is an associate professor of applied mathematics at Columbia University and the Chief Data Scientist at The New York Times.", "summary": "Have questions after a keynote? Excited to learn more from one of our keynote speakers? Fireside chats are a great opportunity meet our speakers in a more intimate setting.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/67/fireside-chat-bauer-wiggins/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "conda-forge sprint", "performer": ["Marius van Niekerk", " Christopher J \"CJ\" Wright"], "@type": "plenary", "description": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!\nSprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "Come work with core developers on conda-forge-related projects. Want to help improve conda-forge infrastructure? Wish a package was more up to date? Want to get a package of your own into the repository? Then this is the sprint for you!", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/106/conda-forge-sprint-2/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:05:00Z", "end": "2019-11-05T10:45:00Z", "duration": "0:40:00"}, {"name": "Every ML Model Deserves To Be A Full Micro-service", "performer": ["Romain Cledat"], "@type": "talk", "description": "At Netflix, data scientists provide value to the business in several areas, from the more typical personalisation aspect to helping determine the type of content to produce or the dates at which said content should be released. Providing this business value from a ML model, however, requires the model to be used in some way, for example, through inference or by integrating its results into some other system (a UI, another service, etc.). Furthermore, ML problems come in all sizes, some will produce models that are used very frequently and continuously, while others provide a very point solution to an important problem but are used once or a very small number of times.\nThis presentation will focus on how, at Netflix, we enable data-scientists to fully own their model, from its inception, training all the way to providing an interface to it by deploying it as a microservice. Our infrastructure allows for the rapid prototyping and iteration between data-scientists and stakeholders as well as the seamless transition between a prototype microservice to a fully scalable and product-quality one if needed. Our infrastructure hits a sweet spot between simplicity and completeness that our data-scientists feel empowered to develop even single-use models due to its simplicity but also confident enough to use it in a more production setting.\nIn this presentation, we describe our infrastructure which is based on Function as a Service (FaaS), and AWS\u2019 S3 and EC2 infrastructure. We will show how we have hit what we believe is an interesting middle ground providing a rich set of scalable functionality to the data-scientist while still allowing them to feel empowered to own their model from inception to a microservice API.", "summary": "The best ML model has little business value if it operates in a vacuum. The infrastructure used at Netflix enables data-scientists to quickly deploy their model as a microservice to perform inference or integrate with other systems. We demonstrate how we leverage a Python environment, familiar to data-scientists, as well as FaaS to remove infrastructure considerations and increase productivity.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/74/every-ml-model-deserves-to-be-a-full-micro-service/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "Dealing With Imbalanced Classes in Machine Learning", "performer": ["Aditya Lahiri"], "@type": "talk", "description": "Outline\nThe talk has the following sections-\n\n\nWhat is Class Imbalance?\nHere we give examples to define what a class imbalanced dataset means and why it should be handled differently.\n\n\nWays to overcome it -\nWe go in detail about 3 ways to tackle the class imbalance problem.\n  a.Sampling\n  b.Setting Hyperparameters to assign weights\n  c.Libraries like imblearn \n\n\nEvaluation Methods\nWe discuss the evaluation methods that best help us judge how our model is performing on an imbalanced dataset.\n\n\nCustom loss\nWe discuss a custom loss function that can considerably better our deep learning model and also explain why it does so.\n\n\nMisc \nWe go over some miscellaneous tricks and steps we can take to avoid common pitfalls.\n     a.Train -  Validation Splits\n     b.Remove classes\n\n\nWhy deal with Class Imbalance?\nOne usually wants a machine learning model to do well on two fronts. The first being the quality of predictions defined by a quantitative metric such as accuracy/precision/recall and the other being the fairness or the logical sanity behind the prediction. Sometimes, these do not go hand in hand. And a major reason behind the model being less accurate or even less fair/logically sound is its inability to deal with imbalanced data. This is where the bias against minority creeps in and our model is no longer a valid reflection of the phenomenon its trying to predict. Hence, it is vital that people have a good understanding of how to mitigate this imbalance in the model.\nWho should listen to this talk?\nFrom ML practitioners who build models to key business decision makers, this is an issue that everyone needs to be aware of. The way one builds their model and the way in which one interprets the model predictions are closely tied to how your model handles skewed imbalanced datasets. Key take-aways from practitioners would be techniques to do better with skewed datasets. Whereas, for decision takers, it will be an insight on how to question the model and the data the model has been trained on.\nAdditional notes\n\nMy name is Aditya Lahiri and I am currently a Machine Learning intern at American Express, Big Data Labs. I am a Computer Science undergraduate from BITS Pilani, Goa and will graduate in December 2019. I love solving problems through data and code. Besides that, I enjoy attending meetups, talks and try my best to contribute to them. I have previously given talks in my college at events like Google Developers Group, Goa.\n\nHere are the slides of this proposal-\nhttps://docs.google.com/presentation/d/1_hiJQsbXHhrzlXxCtPUSpt9-FvMWNlNw1m6cBVPyGCE/edit?usp=sharing", "summary": "Skewed datasets are not uncommon. And they are tough to handle. Usual classification models and techniques often fail miserably when presented with such a problem. We discuss right from the basics of what class imbalance means to how we can overcome it, using various algorithms and some subtle techniques. We discuss details of evaluating our efforts and some small but crucial things that are vital", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/7/dealing-with-imbalanced-classes-in-machine-learning/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "TBD", "performer": ["Akos Furton"], "@type": "talk", "description": "TBD.", "summary": "TBD.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/81/akos-furton-talk/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "High-Performance Data Science at Scale with RAPIDS, Dask, and GPUs", "performer": ["Keith Kraus"], "@type": "tutorial", "description": "As the community of data science engineers approaches problems of increasing volume, there is a prescient concern over the timeliness of their solutions. Python has become the lingua franca for constructing simple case studies that communicate domain-specific intuition; therein, codifying a procedure to (1) build a model that apparently works on a small subset of data, (2) use conventional methods to scale that solution to a large cluster of variable size, (3) realize that the subset wasn't representative, requiring that (4) a new model be used, back to (1), and on it repeats until satisfaction is achieved. This procedure standardizes missteps and friction, whilst instilling within the community the notion that Python is not performant enough to address the great many problems ahead.\nEnter RAPIDS, a platform for accelerating integrated data science. By binding efficient low-level implementations in CUDA C/C++ to Python, and by using Dask's elastic scaling model, a data scientist may now employ a two-step procedure that is many times faster than conventional methods: (1) construct a prototypical solution based on a small subset of data, and (2) deploy the same code on a large cluster of variable size, repeating until the right features are engineered.\nRAPIDS is a collection of open-source libraries fostered by Nvidia, and based on years of accelerated analytics experience. RAPIDS leverages low-level implementations in CUDA, optimizing for massive parallelism, high-bandwidth, and maintaining a focused user-friendly Python interface. Chiefly, we concern ourselves with API parity with respect to Pandas and Scikit-Learn; meaning, a data scientist that knows Pandas and Scikit-Learn will have an easy time getting up to speed with RAPIDS. RAPIDS maintains and contributes to many libraries, including cuDF, a GPU DataFrame library with Pandas parity; cuML, a GPU machine learning library with Scikit-Learn parity; cuGRAPH, a GPU graph library with NetworkX parity; cuXFilter, a GPU cross-filter library, a browser-based cross-filtering solution for visualizing feature data in-memory; and Dask-cuDF, a library for distributed CUDA DataFrame objects. RAPIDS also contributes to libraries for elastic compute and machine learning: Dask, XGBoost, with many more to come.\nBy accelerating the entire ecosystem with CUDA, RAPIDS benefits from incredible acceleration over state-of-the-art CPU implementations and conventional methods. Even better, RAPIDS is committed to the community with its API-parity approach, and with its Apache Arrow compliance. This eliminates inefficient glue-code, and makes it easier for the RAPIDS ecosystem to interoperate with other external libraries.", "summary": "Explore the performance of RAPIDS, an open-source platform for accelerating data science with GPUs. The audience will learn the necessary background to leverage RAPIDS at scale in a real-world problem and get a glimpse of our roadmap.", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/59/high-performance-data-science-at-scale-with-rapids-dask-and-gpus/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/105/unconference-6/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "Introduction to Bayesian Modeling with Stan: No Statistics  Background Required (Pt 1)", "performer": ["Breck Baldwin"], "@type": "tutorial", "description": "Stan is a Bayesian modeling language that enjoys wide adoption across industry and science due to its ability to model complex phenomenon, offer human interpretable simulations and capture uncertainty in an arguably idea way for artificial intelligence and descriptive statistics.\nThis class is for those who don\u2019t know statistics, extremely rusty with statistics or just want a gentle introduction to Bayesian modeling the Stan way. Most importantly we get you through the awkward \u2018no idea phase\u2019 of learning a new technology to having a basic understanding of how to work with the software. We will cover the mechanics of how Stan programs work, show simple Bayesian models and posteriors. We presuppose that you are comfortable with general programming concepts like  subroutines and variable assignment. We will briefly cover Python interfaces to Stan but the majority of the class will be using pure Stan from the command line using a text editor.\nPlease see the Pre/Post test at: https://forms.gle/e3USWaBRsmuz4PDo7 to get a more detailed idea of what we are covering.\nWe will likely have cloud instances of Stan available but those that are comfortable with git should install: https://github.com/stan-dev/cmdstan/wiki/Getting-Started-with-CmdStan", "summary": "This class is for those who don\u2019t know statistics, extremely rusty with  statistics or just want a gentle introduction to Bayesian modeling the Stan way. Most importantly we get you through the awkward \u2018no idea phase\u2019 of learning a new technology to having a basic understanding of how to work with the software.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/80/introduction-to-bayesian-modeling-with-stan-no-statistics-background-required-pt-1/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "conda-forge sprint", "performer": ["Marius van Niekerk", " Christopher J \"CJ\" Wright"], "@type": "plenary", "description": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!\nSprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "Come work with core developers on conda-forge-related projects. Want to help improve conda-forge infrastructure? Wish a package was more up to date? Want to get a package of your own into the repository? Then this is the sprint for you!", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/109/conda-forge-sprint/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T10:55:00Z", "end": "2019-11-05T11:40:00Z", "duration": "0:45:00"}, {"name": "Clean Machine Learning Code:  Practical Software Engineering Principles for ML Craftsmanship", "performer": ["Moussa Taifi Ph.D."], "@type": "talk", "description": "As a community, our work in machine learning inherently depends on external tools and frameworks. However, we have no control over the development and maintenance of these external dependencies. The primary problem is that as a machine learning pipeline become intertwined with a specific ML framework, the harder and more expensive it is to change. This leads ML teams to accumulate technical debt, with serious symptoms like entanglement, hidden feedback loops, undeclared consumers, and pipeline jungles. \nHowever, from a business perspective, Tensorflow, PyTorch, and Scikit-Learn are details. MySQL, EMR, and Hive are details. Airflow, KubeFlow, and Dask are also details. There must be a way to decouple our ML applications from these frameworks and tools. This talk aims to cover the most important Clean Code design principles that can help evolve our ML engineering craftsmanship. \nWe will cover the following goals of a clean machine learning architecture:\n\nLoose Coupling\nHigh Cohesion\nChange is Local\nMake It Easy to Remove\nMind Sized Components\n\nTo achieve those goals we will dive into the clean code design principles, and explain how they relate to common ML tasks and components:\n\nSingle Responsibility Principle (SRP)\nOpen-Closed Principle (OCP)\nInterface Segregation Principle (ISP)\nDependency Inversion Principle (DIP)\n\nIt is well accepted that a good architecture maximizes the number of decisions not made. Creating good architecture requires extensive experience in the target domain. However, as of 2019, 40% of data scientists in the USA have less than 5 years of experience. This inexperienced workforce does not make these challenges any easier. At the same time, we are experiencing a boom in ML development and usage. This is similar to previous software engineering expansions in the 2000s. The current expansion manifests itself with a menagerie of constructs, frameworks, and workflows. This creates a multitude of integration challenges that remind us of good old software engineering problems. Some challenges of ML engineering are indeed new. However, the majority of the software engineering concerns have a historical smell. Going back to the basics of good software engineering can help with today\u2019s ML engineering problems.\nThis talk will help the audience apply the principles of clean machine learning code, and escape the vicious cycle of ML technical debt.", "summary": "Machine learning pipelines are software pipelines after all. Their complexity and design viscosity lead to spectacular, costly and even deadly ML failures. This talk describes the most important Clean Code and Clean Architecture design principles, applied to machine learning applications. It aims to help the audience reduce machine learning technical debt, and to design robust ML architectures.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/20/clean-machine-learning-code-practical-software-engineering-principles-for-ml-craftsmanship/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T11:40:00Z", "end": "2019-11-05T12:20:00Z", "duration": "0:40:00"}, {"name": "Discover your latent food graph with this 1 weird trick", "performer": ["Alex Egg", " Emily A Ray", " Parin Choganwala"], "@type": "talk", "description": "Discovery and Understanding of a product catalog is an important part of any e-commerce business. Learning product interactions by building taxonomies is the traditional, however, difficult method of catalog understanding. At Grubhub we leverage recent advancements in Representation Learning, namely Sequential Modeling and Language Modeling to learn a Latent Food Graph. With our strong and scalable understanding of the product catalog, we power better search and recommendations in a much more sustainable fashion than maintaining an expensive handmade taxonomy. \nBy integrating pyspark and tensorflow into our end to end pipeline, we create a scalable service that spans the data collection and exploration phases and culminates in visualization tools and models served for real time use in our production environment. \nWe hope to highlight 3 recently successful projects:\n\nRest2Vec: Embedding Restaurants w/ behavioural clickstream data\nQuery2Vec: Embedding Search Queries w/ behavioural clickstream data\nFastMenu: Language Model learnt on content-based menu text w/ location bias\n\nWith these 3 techniques we have a strong semantic understanding of the food graph, for example: \n\nWhat are 3 semantically similar restaurants to Le Prive NYC? (cuisine-level semantics) \nWhat are some related cuisines to Udon Noodles? (cuisine graph traversal)\nWhat are menu items similar to Blueberry Pancake Breakfast? (semantic matching)\nWhat are some synonyms for Pierogi? (cross-lingual query expansion) \nWhat are 3 dishes similar to Kimchi-jjigae? (dish-level semantics)\n\nTopics: word2vec, object2vec, query expansion, semantic matching, personalization, search", "summary": "At Grubhub we leverage recent advances in Representation Learning to gain an automated and scalable understanding of our vast restaurant and menu catalog. We use these techniques to learn a latent food knowledge graph in order to drive better search and personalization. Particularly, we hope to share some of our advances in using: language modeling and knowledge graphs in the e-commerce setting.", "level": "Experienced", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/54/discover-your-latent-food-graph-with-this-1-weird-trick/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T11:40:00Z", "end": "2019-11-05T12:20:00Z", "duration": "0:40:00"}, {"name": "Colorism in High Fashion (featuring: K-Means Clustering)", "performer": ["Malaika Handa"], "@type": "talk", "description": "Prejudice against people of color with a darker skin tone (called \u201ccolorism\u201d) is prevalent in high fashion, but because it is subtler than overt racism it is harder to quantify. This talk will go into the technical details of transforming twenty years of Vogue cover photos into a numeric dataset that measures skin tone lightness, which, when visualized, showcases the striking lack of diversity on the cover of Vogue magazine.\nThis spring, I worked with The Pudding to explore diversity in fashion through the lens of colorism. We applied quantitative methods to an area where research is overwhelmingly qualitative by developing a metric that measures how light the skin of a Vogue model looks in a photograph.\nThe result was a case study that went pretty viral. The article itself skimmed over the techniques that were used to gather data, so I will speak about the intricacies of building this dataset:\n\nInterpreting a photograph as a three-dimensional array of floats\nUsing HSL color representation to extract meaningful features from a photograph\nFitting and tuning k-means models to identify skin in a photograph\nUsing human-in-the-loop machine learning to improve model results\nVisualizing color data\n\nThis talk will be accessible to people with a beginner level knowledge of machine learning, and no background in fashion.", "summary": "This talk will go into the technical details of transforming twenty years of Vogue cover photos into a numeric dataset that measures skin tone lightness, which, when visualized, showcases the striking lack of diversity on the cover of Vogue magazine.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/43/colorism-in-high-fashion-featuring-k-means-clustering/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T11:40:00Z", "end": "2019-11-05T12:20:00Z", "duration": "0:40:00"}, {"name": "Stars, Planets, and Python", "performer": ["Sara Seager"], "@type": "discussion", "description": "For thousands of years people have wondered, \u201cAre there planets like Earth?\u201d \u201cAre such planets common?\u201d \u201cDo any have signs of life?\u201d Today astronomers are poised to answer these ancient questions, having recently found thousands of planets that orbit nearby Sun-like stars, called \u201cexoplanets\u201d. Professor Sara Seager, one of the world\u2019s leading experts on this search for Earth-like planets, will share the latest advances in this revolutionary field.", "summary": "Keynote by Professor Sara Seager, an astrophysicist and planetary scientist at the Massachusetts Institute of Technology. She works on exoplanets, planets that orbit stars other than the sun.", "level": "Novice", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/64/keynote-sara-seager/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T13:20:00Z", "end": "2019-11-05T14:10:00Z", "duration": "0:50:00"}, {"name": "tf-explain: Interpretability for Tensorflow 2.0", "performer": ["Rapha\u00ebl Meudec"], "@type": "talk", "description": "We will explore some research papers on interpretability of neural networks, at different scale: from the ultra-specific with analysis of convolutional filters to more user-friendly input visualizations.\nFor each method, I'll provide some theoretical explanations (what mathematical operations we are performing), and a Tensorflow 2 implementation to examine in details how to proceed.\nFinally, we will go through tf-explain usage, from offline model inspection to training monitoring.\nRoadmap :\n\nConvolutional Kernel Filter Visualization\nSaliency Maps (Vanilla Gradients, SmoothGrad)\nClass Activation Maps\nOcclusion Sensitivity\nTF-explain Usage", "summary": "Deep learning models now emerge in multiple domains. The question data scientists and users always ask is \"Why does it work?\". Explaining decisions from neural networks is vital for model improvements and analysis, and users' adoption. In this talk, I will explain interpretability methods implementations with TF2.0 and introduce tf-explain, a TF2.0 library for interpretability.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/11/tf-explain-interpretability-for-tensorflow-20/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Type-Driven Automated Learning with Lale", "performer": ["Martin Hirzel"], "@type": "talk", "description": "When writing machine-learning pipelines, you have a lot of decisions to make, such as picking transformers, estimators, and hyperparameters. Since some of these decisions are tricky, you will likely find yourself searching over many possible pipelines. Machine-learning automation tools help with this search. Unfortunately, each of these tools has its own API, and the search spaces are not necessarily consistent nor even correct. We have discovered that types (such as enum, float, or dictionary) can both check the correctness of, and help automatically search over, hyperparameters and pipeline configurations. This talk presents Lale, an open-source Python library for semi-automated data science. Lale is compatible with scikit-learn, adding a simple interface to existing machine-learning automation tools. Lale lets you search over possible pipelines in just a few lines of code while remaining in control of your work.", "summary": "This talk presents Lale, an open-source Python library for semi-automated data science. Lale is compatible with scikit-learn, adding a simple interface to existing machine-learning automation tools. Lale lets you search over possible pipelines in just a few lines of code while remaining in control of your work.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/29/type-driven-automated-learning-with-lale/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Data-centric exploration using intake, dask, hvplot, datashader, panel, and binder", "performer": ["Julia Signell"], "@type": "talk", "description": "In this talk we will start with data files, ingest the data, do a quick first pass at visualization and end at a deployed dashboard, using a set of open-source packages that have been designed to work well together. In particular, the audience will learn to:\n\ningest data from almost any file format with intake\neven large data thanks to dask\nquickly inspect pandas, xarray, or streaming data with hvplot\ninteract with plots in web browsers using bokeh\nplot large datasets with datashader\ncreate dashboards with panel\ndeploy dashboards on binder\n\nThis might seem ambitious, but many of these packages are used behind the scenes, so you don't have to learn how to use a new package.", "summary": "Discovering a new dataset is exciting, but it can also be daunting. How will you read it? Will it fit in memory? What meaning can be derived? Can you share that with others? \nThis talk describes an opinionated method of data exploration going from data ingestion to dashboard deployment and showing how everything comes together into a complete and repeatable workflow.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/48/data-centric-exploration-using-intake-dask-hvplot-datashader-panel-and-binder/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Genetic algorithms: Making errors do all the work", "performer": ["Raman Tehlan"], "@type": "talk", "description": "Outline\nOrigin [5 Minutes]\n\nSpeaker introduction;\nTalk Introduction;\nOrigin and Inspiration behind GA;\n\nTheory [10 Minutes]\n\nAlgorithm Structure;\nInitial Population;\nSolution(Chromosome, Genes);\nFitness Function;\nCrossover;\nMutation;\nTermination;\n\nHands-on [15 Minutes]\n\nLive code a Genetic Algorithm to make car travel to the finish line without crashing;\nPlay with mutation(errors) to see how it change the solution.\nWhen to use GA;\nAdvantages and Limitations of GA;\n\nEnding [10 Minutes]\n\nClosing Note;\nkey Takeaway;\nQ/A\n\nSummary\nOrigin\nGenetics has been the root behind the life today, it all started with a single cell making an error when dividing themselves. It was discovered by Charles Darwin & Alfred Russel Wallace in their book \"On the Origin of Species\", which highlighted 2 important points:\n\nSurvival of the fittest\nMutation.  \n\nThis later inspired John Henry Holland to create Genetic Algorithms and write about it in his book \"Adaptation in Natural and Artificial Systems\"\nTheory\nIn computer science, Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems. To understand this better we will start with an example by creating a tiny world of cars(solution), and along the way discuss the definitions.\n\nDecide the size of the population: N = 10\nTo live, cars have to solve the problem of crossing a road without touching the borders.\nDecide the characteristics(genes) of a single individual(Chromosome).\n\n{\n  \"id\": \"0-n\",\n  \"length\": \"Length of car\",\n  \"width\": \"Width of car\",\n  \"height\": \"Height of car\",\n  \"step-size\": \"Speed\",\n  \"angle\": \"Angle at which car is driven\",\n  \"step-taken\": \"Steps taken before termination; set after solution is implemented\",\n  \"fitness\": \"will be between 0-1; set after the solution is implemented\"\n}\n\n\nCreate 10 cars with a 10 random chromosomes.\n\n{\n  \"id\": \"0\",\n  \"length\": \"60\",\n  \"width\": \"40\",\n  \"height\": \"50\",\n  \"step-size\": \"10\",\n  \"angle\": \"30\",\n  \"step-taken\": \"0\",\n  \"fitness\": \"-1\"\n}\n...\n\n\nMake these cars cross the road.\nFind the fitness of cars, which is how far a car was able to travel from the start.\n\n\"\"\"\n Both angel and step-taken can be used for fitness, but angle won't be a good\n criteria cause it is only applicable if there are no other cars on the road.\n However, but for our case we will use it.\n\"\"\"\ndef calculateFitness(carN):\n    fitness = angleNearZero(carN.angle)\n\n\nCrossover top 5 cars to create 10 new cars.\nMutate some of the chromosomes.\nMake these cars cross the road again\nRepeat above steps until all the cars can cross the road.\n\nGenetic_Algo ()\n\n    initialize_population ()\n        for(Population size)\n            {\n                gene1: value,\n                gene2: value\n            }\n\n   while (Termination) do\n      Solve the problem\n      Fitness()\n\n      parents selection()\n      crossover()\n            {\n                gene1: value of parent 1,\n                gene2: value of parent 2\n            }\n\n      mutation()\n   return best\n\n\nHands-on\nHere I will live code the above example from scratch, it will most likely be in Jupiter notebook with graphics to help the audience understand better.\nKey Takeaways\nWriting basic Genetic Algorithm\nThis talk will give enough information and methods to the audience that they will be\nable to program their simple Genetic Algorithm, plus they will also be able\nto research the topic further with little or no help.\nMutation is actually an error\nWithout mutation(error) life is just Permutation and Combination problem,\nits this error which open infinite possibilities.", "summary": "This talk presents a systematic approach to understand and implement Genetic Algorithms, with a hands-on experience of solving a real-world problem. The inspiration and methods behind GA will also be included with all the fundamental topics like fitness algorithms, mutation, crossover etc, with limitations and advantages of using it.", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/77/genetic-algorithms-making-errors-do-all-the-work/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Fireside Chat", "performer": ["Sara Seager"], "@type": "discussion", "description": "A fireside chat with keynote speaker Sara Seager.\nProfessor Sara Seager is an astrophysicist and planetary scientist at the Massachusetts Institute of Technology. Click her name above to learn more!", "summary": "Have questions after a keynote? Excited to learn more from one of our keynote speakers? Fireside chats are a great opportunity meet our speakers in a more intimate setting.", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/68/fireside-chat-sara-seager/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Introduction to Bayesian Modeling with Stan: No Statistics  Background Required (Pt 2)", "performer": ["Breck Baldwin"], "@type": "tutorial", "description": "Stan is a Bayesian modeling language that enjoys wide adoption across industry and science due to its ability to model complex phenomenon, offer human interpretable simulations and capture uncertainty in an arguably idea way for artificial intelligence and descriptive statistics.\nThis class is for those who don\u2019t know statistics, extremely rusty with statistics or just want a gentle introduction to Bayesian modeling the Stan way. Most importantly we get you through the awkward \u2018no idea phase\u2019 of learning a new technology to having a basic understanding of how to work with the software. We will cover the mechanics of how Stan programs work, show simple Bayesian models and posteriors. We presuppose that you are comfortable with general programming concepts like  subroutines and variable assignment. We will briefly cover Python interfaces to Stan but the majority of the class will be using pure Stan from the command line using a text editor.\nPlease see the Pre/Post test at: https://forms.gle/e3USWaBRsmuz4PDo7 to get a more detailed idea of what we are covering.\nWe will likely have cloud instances of Stan available but those that are comfortable with git should install: https://github.com/stan-dev/cmdstan/wiki/Getting-Started-with-CmdStan", "summary": "This class is for those who don\u2019t know statistics, extremely rusty with  statistics or just want a gentle introduction to Bayesian modeling the Stan way. Most importantly we get you through the awkward \u2018no idea phase\u2019 of learning a new technology to having a basic understanding of how to work with the software.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/78/introduction-to-bayesian-modeling-with-stan-no-statistics-background-required-pt-2/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/112/unconference-11/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:10:00Z", "end": "2019-11-05T14:55:00Z", "duration": "0:45:00"}, {"name": "Building a maintainable plotting library", "performer": ["Colin Carroll", " Hannah Aizenman", " Thomas Caswell", " Thomas Caswell"], "@type": "talk", "description": "There is something compelling about a data visualization that lets us explore a data set, comprehend an analysis, or think about a concept.  A common desire for companies, labs, open source projects, and weekend hacks is to have a collection of easy-to-use visualizations that \u201cdo the right thing\u201d in the context of your data. In the larger Python data science stack,  this means that the visualizations must play nicely with matplotlib (or another exciting visualization library!), and probably libraries like numpy, pandas, and xarray.\nFirst we will show you how to use a custom stylesheet, which is a low-effort, high-return way of having a distinctive, stylish, and uniform look by default. \nFor stand-alone plotting functions and quick projects, we give a few tips and best practices: how to get and update your figure and axes, what sort of arguments to accept and return, and how to write unit tests.\nWe also who how to about implement custom performant functionality using Artists. There are a few built-in artists that the common matplotlib  functions are built with, and comfort with these may be particularly useful for library authors, or when you find you are reusing certain patterns in multiple visualizations. \nWe conclude with a discussion of building a custom Artist, which is useful when performance becomes an issue, when you expect other functions may want to use it, or when you really need to do something extra creative.", "summary": "Fluid data visualization needs to be aware of the explicit and implicit structure of the data you want to visualize. Whether you are working on a personal project, or want to have a consistent set of figures for your company or lab, we will share tips on how to build a custom library using matplotlib that is easy to use, easy to maintain, easy to extend, and customized to your use case.", "level": "Novice", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/49/building-a-maintainable-plotting-library/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:55:00Z", "end": "2019-11-05T15:35:00Z", "duration": "0:40:00"}, {"name": "Implementing Lightweight Random Indexing for Polylingual Text Classification", "performer": ["Ian Whalen"], "@type": "talk", "description": "Description:\n\nText classification is the complex task of assigning labels to natural language. Polylingual text classification extends this task by requiring a modeling approach to assign labels to corpi with multiple languages. Modeling each language separately may cause overfitting to datasets with too few data points. To remedy this, we present Lightweight Random Indexing, a method that can deal with large corpi and project multiple languages into the same space.\nRandom indexing is a dimensionality reduction method that projects data points into a lower dimensional space while preserving the distance between points. This process often involves performing a large matrix multiplication which can be infeasible on some datasets. Lightweight random indexing introduces a clever dictionary trick to store mappings of vectors to a subspace using significantly less memory. We will present all of the above topics and their implementations in Python using common libraries.\nOutline:\n\n\nIntroduction (2 minutes; 2 total)\nQuick Bio on myself\n\nAgenda of talk\n\n\nProblem Definition (5 minutes; 7 total)\n\nText classification refresher\nText representation refresher (BoW, TFIDF, word vectors)   \n\nPolylingual setting\n\n\nNaive Approach (3 minutes; 10 total)\n\nModel for each language\n\nDrawbacks of this approach\n\n\nRandom Indexing (8 minutes; 18 total)\n\nIntroduction to random indexing\nSome theory around the topic + intuition\n\nPython implementation\n\n\nProblem with Vanilla Random Indexing (1 minute; 19 total)\n\n\nLightweight Random Indexing (4 minutes; 23 total)\n\nApproach description\nTheoretical justification\n\nPython implementation\n\n\nEmpirical Results (2 minutes; 25 total)\n\nHow well does it work?\nResults from paper\n\nResults on a baseline, TFIDF on 1 dataset vs. a model trained with another language's embeddings\n\n\nQuestions (5 minutes; 30 total)\n\n\nAdditional Notes:\n\nWe have a working implementation that can be cleaned up and shared publicly before the talk.", "summary": "Most NLP tasks offer a simple setup: one language and a target. In the real world, we may face with the fact that people speak different languages when building models. This talk will motivate the use of lightweight random indexing (see paper here) to combine data sets across multiple languages and walk through a working implementation.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/16/implementing-lightweight-random-indexing-for-polylingual-text-classification/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:55:00Z", "end": "2019-11-05T15:35:00Z", "duration": "0:40:00"}, {"name": "Should I develop my own DS library? Maybe.", "performer": ["Piero Ferrante"], "@type": "talk", "description": "Have you ever entertained the idea of developing your own library for data science related tasks to be used among your team or organization? If so, phrases like \"don't recreate the wheel\" also probably came to mind. This talk will address compelling arguments on both sides of the topic while telling the story of why we at Aetna ultimately decided to do just that and what we learned along the way.", "summary": "Have you ever entertained the idea of developing your own library for data science related tasks to be used among your team or organization? If so, phrases like \"don't recreate the wheel\" also probably came to mind. This talk will address compelling arguments on both sides of the topic while telling the story of why we at Aetna ultimately decided to do just that and what we learned along the way.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/36/should-i-develop-my-own-ds-library-maybe/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:55:00Z", "end": "2019-11-05T15:35:00Z", "duration": "0:40:00"}, {"name": "How and why to put your Jupyter notebooks in Docker containers", "performer": ["Brian Austin"], "@type": "talk", "description": "Presenting a Jupyter notebook with your finished work is good, but what if you want to allow someone to play with the finished product themselves? Sure, you could include README instructions on how to install requirements, include a link to download and process the data, and include pickled models in a persistent file storage, but that introduces a lot of room for error. In this talk, we will walk through the process for building a Docker container that will include processed data, will pre-run models, and allow a user to explore the resulting outputs in a familiar Jupyter environment.\nWe will discuss the ready-to-run Docker images from the Jupyter project, and some of the limitations that mean you might want to write and push your own images. Attendees of the talk will get a working and well-commented Dockerfile that builds a machine learning model and packages the data up for exploration, as well as a cheat sheet of useful commands for working with containers in a Jupyter environment.\nNote: No Docker experience is required for this talk, but an installed Docker instance and account on DockerHub may be useful.", "summary": "Jupyter notebooks are a key part of a data scientist's professional output. Learn how to go from a notebook that shows a report to a reproducible service with the help of Docker. In this short walkthrough, you'll learn how to write a Dockerfile that packages your work effectively and consistently, so that it can be shared with any audience.", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/72/how-and-why-to-put-your-jupyter-notebooks-in-docker-containers/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:55:00Z", "end": "2019-11-05T15:35:00Z", "duration": "0:40:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/107/unconference-8/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T14:55:00Z", "end": "2019-11-05T15:35:00Z", "duration": "0:40:00"}, {"name": "The Secret Life of Python", "performer": ["Steve Dower"], "@type": "talk", "description": "Python 3.8 is about to be released, and hackers are worried. For years, being able to access Python on someone else's machine has meant freedom, power, immunity, and most importantly, reliable cross-platform malware. Some of the most powerful hacking tools are written in Python, and it needs to end.\nWhile the bad guys are worried, the good guys are celebrating. Python 3.8 is bringing some new features through auditing events and the verified open hook. No longer will Python be a black box within your system, but the log stream available through auditing hooks will let you know whenever it is being (mis)used. Being able to verify code files before they are executed allows administrators to detect tampering and ensure the integrity of their system, rather than allowing attackers to infiltrate and persist.\nIn this session, we will see why hackers love the power and flexibility of Python. We will implement actual auditing hooks to expose their secret acts, and see some of the operating system features that are now available to Python developers. Rather than being a vulnerability in your environment, these features make it an early warning system, helping protect you and your users from the biggest threats in our always-connected world.", "summary": "Python 3.8 includes new features for security engineers to allow greater insight and awareness when your network is under attack (which is every day). This session will investigate the need for these hooks and how they are used to protect your production servers and your users.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/45/the-secret-life-of-python/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "Free Your Esoteric Data Using Apache Arrow and Python", "performer": ["Maciej Wojton"], "@type": "talk", "description": "One common scenario in large enterprise systems is esoteric/inconsistently structured data. This data is crucial to a firm\u2019s success, but cannot be easily read, analyzed or extracted. The data might not have a schema and might only exist in memory. An example of this is C++ code that has classes with strange and inconsistent interfaces which do not have fast human-readable serializations. Programmers are stuck when needing to test and analyze this data. A better solution would be to migrate the data to a common schema-based format and use Python data science libraries to analyze it.\nPython (with clang) and Apache Arrow enables you to quickly and easily transform data into the Apache Parquet format, where you can use PyArrow and pandas to analyze it. Attendees will learn how to:\n\nUse PyArrow to read CSV, JSON, custom data and hierarchical data.\nUse pandas to elegantly compare data.\nParse their C++ classes using the cindex module and extract all the relevant data accessors and generate a \u201cschema.\u201d\nUse the schema with a Jinja template to convert from C++ to Parquet using the Apache Arrow C++ API.\n\nOutline\nProblem statement\n\nGo over the problem and show what esoteric data is.\n\nPandas\n\nReview pandas role in comparing esoteric data.\n\nFree Data!!\nApache Arrow Overview\n\nGo over examples of PyArrow and why you want to use Apache Arrow.\nHow to read CSV, JSON, custom and hierarchical data and save it to Parquet.\n\n\n\nPut Clang, Jinja, Arrow and Python together to access your data:\n\nReview a specific example of how to parse C++ classes using Python\u2019s cindex module\nCreate the schema from the parsing\nGenerate the converter using Jinja templates\nconvert\n\n\nShow details of the generated Parquet file", "summary": "Do you work with esoteric data that has no schema, no human-readable output, and/or inconsistent interfaces? Is your data only readable from C++ classes with a secret encoding? Let me demonstrate how to use Python and Apache Arrow to quickly read your data into pandas and elegantly analyze the data.", "level": "Intermediate", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/61/free-your-esoteric-data-using-apache-arrow-and-python/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "The Inspection Paradox is Everywhere", "performer": ["Allen Downey"], "@type": "talk", "description": "When you drive on a highway, have you ever thought that everyone drives too fast or too slow, and no one else is a safe, reasonable driver like yourself?  If so, you have been fooled by the inspection paradox, an often subtle form of biased sampling.\nIn this talk, I explain the inspection paradox with examples from social networks, transportation, education, incarceration, and other domains.  Using simple Python code, I show how the effect can be quantified and how, with clever experimental design, you can take advantage of it.\nThis talk is accessible for people who know basic Python and minimal statistics.", "summary": "The inspection paradox is a statistical illusion you\u2019ve probably never heard of. It\u2019s a common source of confusion, an occasional cause of error, and an opportunity for clever experimental design.  And once you know about it, you see it everywhere.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/26/the-inspection-paradox-is-everywhere/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "Painting A Picture of Public Data", "performer": ["Kamal Abdelrahman"], "@type": "talk", "description": "The talk is a walkthrough analysis of New York open data, a representation of the way data scientists can be more active in the public sector. Datasets will be pulled from public data portals, illustrating psychologist Abraham Maslow\u2019s hierarchy of needs. The aim will be to establish that the cardinal and common needs among all humans is to have that sense of fulfillment with shelter, safety, belonginess, and prestige. These basic needs transcend every way a person is classified. Driven by data science tools of visualizations and analytics, we begin to realize there is no significant difference in self-actualization regardless of the variation within our external differences.", "summary": "Data powers society, information that changes situations. How can we use data to help people understand the world they live in? With the use of the publicly available data, this talk aims to see past our external identities of our jobs, physical traits, and even geographical origin. This talk is a story of New York, narrated by data science, written in code, illustrated with art.", "level": "Intermediate", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/38/painting-a-picture-of-public-data/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "Unconference", "performer": ["YOU!"], "@type": "plenary", "description": "What's an \"Unconference\"?\nPyData NYC draws open source scientific computing enthusiasts from across industries and experience levels. Often the most valuable parts of a conference are the informal conversations that happen between people in different teams or roles or industries who don\u2019t normally work together. People get to know each other, exchange knowledge, and build trust.\nThe Unconference format encourages and prioritizes these interpersonal interactions this by providing just enough structure to communicate what is happening and where (encouraging broad participation), however, the agenda is driven by participant interest and topic relevance.\nWhat are the four principles of Unconferencing?\n\nWhoever comes are the right people.\nWhatever happens is the only thing that could.\nWhenever it starts is the right time.\nWhen it is over it is over.\n\nWhat is the format of an Unconference session?\nThe format is whatever you want it to be! Presumably it should be about the topic you chose, but other than that, you have 40 to 50 minutes (depending on time slot) to do as you please. You're free to rope in other people for a panel discussion, give a demo, give a traditional presentation, whatever. You're encouraged to include the audience through Q&A, open discussions, interactive demos, or other means.\nHow do I sign up to lead an Unconference session?\nAnyone can lead (or co-lead) a session, and there is no deadline (except the end of the conference) for signing up. \nThere are two ways to nominate a topic: You can head over to the #unconference Slack channel, and post the topic you want to lead a session on as a message. A PyData volunteer will reach out to you about scheduling, and others may reach out about participating as well or react to show interest. Alternatively, when you get on-site at the conference, there will be an easel posted outside of the Unconference room where you can write down your ideas as they come to you (along with your name). As in the other case, a volunteer will reach out to you about scheduling.\nDo I have to choose between attending the conference or the Unconference?\nNo, these are parallel tracks that are intended to supplement one another. You have access to both as part of your PyData NYC ticket. Feel free to move back and forth between the two.\nWhat is the Law of 2 Feet?\n\u201cIf you aren\u2019t contributing or learning or having fun where you are now, use your two feet.\u201d\nIs this recorded?\nNo. Feel free to write up and share key discussion points with the conference volunteers who will be documenting this event in other ways.", "summary": "In the Unconference track, the conference is generated by YOU!  The schedule for this event will be developed on-site and continue to evolve throughout the event. Bring your ideas for discussion topics or form breakout groups ad hoc. These are similar to Birds of a Feather rooms hosted at previous PyData events.", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/108/unconference-9/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "Introduction to Bayesian Modeling with Stan: No Statistics  Background Required (Pt 3)", "performer": ["Breck Baldwin"], "@type": "tutorial", "description": "Stan is a Bayesian modeling language that enjoys wide adoption across industry and science due to its ability to model complex phenomenon, offer human interpretable simulations and capture uncertainty in an arguably idea way for artificial intelligence and descriptive statistics.\nThis class is for those who don\u2019t know statistics, extremely rusty with statistics or just want a gentle introduction to Bayesian modeling the Stan way. Most importantly we get you through the awkward \u2018no idea phase\u2019 of learning a new technology to having a basic understanding of how to work with the software. We will cover the mechanics of how Stan programs work, show simple Bayesian models and posteriors. We presuppose that you are comfortable with general programming concepts like  subroutines and variable assignment. We will briefly cover Python interfaces to Stan but the majority of the class will be using pure Stan from the command line using a text editor.\nPlease see the Pre/Post test at: https://forms.gle/e3USWaBRsmuz4PDo7 to get a more detailed idea of what we are covering.\nWe will likely have cloud instances of Stan available but those that are comfortable with git should install: https://github.com/stan-dev/cmdstan/wiki/Getting-Started-with-CmdStan", "summary": "This class is for those who don\u2019t know statistics, extremely rusty with  statistics or just want a gentle introduction to Bayesian modeling the Stan way. Most importantly we get you through the awkward \u2018no idea phase\u2019 of learning a new technology to having a basic understanding of how to work with the software.", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/79/introduction-to-bayesian-modeling-with-stan-no-statistics-background-required-pt-3/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "PyData Pop Quiz", "performer": ["James Powell"], "@type": "plenary", "description": "Test your knowledge of Python and the NumFOCUS/PyData stack against the best and brightest in NYC. Brought to you by PyData Vice President of Quizmastering, James Powell.", "summary": "The famed PyData Pop Quiz returns to PyData NYC!", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/93/pop-quiz/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T15:45:00Z", "end": "2019-11-05T16:30:00Z", "duration": "0:45:00"}, {"name": "Reproducibility in ML Systems: A Netflix Original", "performer": ["Ferras Hamad"], "@type": "talk", "description": "As more machine learning initiatives developed into business critical systems it became increasingly important to provide a means of guaranteeing some degree of reproducibility around them. Although reproducibility is often talked about as a binary construct, either something is reproducible or it isn\u2019t, in practice when it comes to machine learning is best described as a spectrum. It is usually very expensive and difficult to make everything completely reproducible in every situation but often times even offering some level of reproducibility can yield great short-term and long-term benefits.\nWith the high degree of reproducibility guaranteed by the approach described in this talk we were able to extract multiple benefits such as rapid prototyping, improved debugging, lower barriers to collaboration, and better reliability. This has allowed scheduled workflows to run continuously for months and years with little to no human intervention and reduced the engineering footprint required for each data science related project.", "summary": "In this talk, we focus on the importance and benefits of effortless reproducibility as a first-class construct in machine learning infrastructure. From practices around capturing inputs and outputs to code versioning and transitive dependency management.", "level": "Intermediate", "room": "Central Park West (6501)", "url": "https://pydata.org/nyc2019/schedule/presentation/53/reproducibility-in-ml-systems-a-netflix-original/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T16:30:00Z", "end": "2019-11-05T17:15:00Z", "duration": "0:45:00"}, {"name": "Sloth & ENVy", "performer": ["James Powell"], "@type": "talk", "description": "\"Don't show up to this talk, it's going to be stupid. You won't learn anything, and if you do learn something, I promise it will be dumb.\" - Diego Torres Quintanilla, PyData NYC Executive Chair", "summary": "This will be James Powell's talk", "level": "Novice", "room": "Central Park East (6501a)", "url": "https://pydata.org/nyc2019/schedule/presentation/76/tbd/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T16:30:00Z", "end": "2019-11-05T17:15:00Z", "duration": "0:45:00"}, {"name": "A How-to guide for migrating legacy data applications", "performer": ["Marius van Niekerk", " Rohit Kapur"], "@type": "talk", "description": "Data applications are no different from regular software and accrue technical debt like all applications over time. They frequently receive little updates because \u201cthey work\u201d and continue to provide business value. Over time, they end up being viewed as black boxes for producing valuable data. \nThere\u2019s often an inherent tension between users of the data application who are primarily concerned with continuing to produce data reliably in the short term and the teams responsible for the evolution of the data application who are concerned with adapting it to gracefully serve the business as it scales.\nWe will discuss moving out such an application based on a case study of an application that we have migrated.\nWe cover the following key regions\n\nShould you even do this?\nWhere are you migrating from? Where to?\nWhat do you gain in the new world? What do your customers gain?\nBuild it from scratch? (every developer\u2019s tendency)\nDo you feel good about owning the system in its current state?\n\n\nDefining a project plan\nHow do you break into phases? What does success look like for each?\n\n\nAutomating as much as you can\nShowing that this will work\nAssuaging fears and messaging\nHarvesting production for tests.\n\n\nDoing this without disrupting business operations\nGetting buy-in from customers\nOffering \u201ccarrots\u201d along the way\n\n\n\nThis talk will be relevant for members of teams who own such applications, but the lessons learnt can help developers avoid building up unmaintainable software. Familiarity with any existing tools or software is not required for this talk.", "summary": "Migrating legacy applications can be daunting. They are frequently large, poorly tested, undermaintained, yet business critical. Over time they may end up relying on deprecated technology and unprepared for changing business needs. We\u2019ll cover lessons learned in migrating such an application to something maintainable, sharing details on how to go about it in a safe and systematic manner.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/56/a-how-to-guide-for-migrating-legacy-data-applications/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T16:30:00Z", "end": "2019-11-05T17:15:00Z", "duration": "0:45:00"}, {"name": "Building Software and Communities With Peer Review", "performer": ["Noam Ross"], "@type": "talk", "description": "Code review is a common practice in software engineering, and peer review is a key mechanism of quality control and validation in scientific publication.  Peer review of scientific code, however, is rare.  rOpenSci, a developer's collective that creates software to support scientific reproducibility, has developed a system for software peer review that has enabled us to build a trusted collection of over 120 software packages in the past four years.\nIn this presentation I will describe our review approach and the lessons we've learned, including the surprising outcomes and benefits of fully open review. Software peer review not only enables quality control: we have found it an excellent mechanism for seeding new collaborations and communities, for building consensus on standards, and pushing best practices out into a community much wider than our own authors and reviewers. These benefits come from careful design and maintenance of both technical and social systems.  I will discuss these designs and how they can be integrated into other projects and communities, and explain how new authors and reviewers can navigate software peer-review in projects such as rOpenSci, pyOpenSci, or the Journal of Open-Source Software.\nThis talk is for anyone interested in getting involved in software peer review as an author or a reviewer, or bringing software peer review to their own teams.", "summary": "Software peer review can improve software quality, accelerate the adoption of best practices and community standards, and build communities of practice.  I will present lessons from over four  years of software peer review at rOpenSci, and new initiatives such as pyOpenSci that are expanding software peer review into new fields and languages.", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/73/building-software-and-communities-with-peer-review/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T16:30:00Z", "end": "2019-11-05T17:15:00Z", "duration": "0:45:00"}, {"name": "Stump the Chump", "performer": ["Thomas Caswell", " Anthony Scopatz", " Paul Ganssle"], "@type": "plenary", "description": "Special guests Tom Caswell (matplotlib), Paul Gannsle (dateutil) and Anthony Scopatz (xonsh) will solve challenges proposed by the audience using their own libraries! Come see whether you can stump these chumps!", "summary": "Three core devs. No consulting the docs.", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/92/stump-chump/", "date": "Tuesday Nov. 5, 2019", "start": "2019-11-05T16:30:00Z", "end": "2019-11-05T17:15:00Z", "duration": "0:45:00"}, {"name": "Introduction to pandas", "performer": ["Marc Garcia", " Jeff Reback", " Tom Augspurger"], "@type": "tutorial", "description": "This tutorial is aimed at people new to pandas. Knowledge of Python will be useful but is not essential. JupyterLab will be used, and attendees are expected to complete short exercises during the tutorial.\nThe tutorial will start by the basic pandas features, but it will go deep into some of the pandas implementation details, so attendees are prepared to deal with real data.\nWe will cover the next topics:\n\nLoading data from different formats\nTransforming data and data types\nJoining datasets and reshaping\nFiltering and selecting\nDealing with missing values\nAggregating data and computing statistics\nData visualization", "summary": "This tutorial is a hands on introduction to pandas. pandas is a fast, powerful, flexible and easy to use data analysis and manipulation tool, built on top of the Python programming language.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/32/introduction-to-pandas/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T09:00:00Z", "end": "2019-11-06T10:30:00Z", "duration": "1:30:00"}, {"name": "An Introduction to Probability and Statistics", "performer": ["Will Kurt"], "@type": "tutorial", "description": "Introduction to Probability and Statistics\nSpeaker: Will Kurt\nAudience: Anyone interested in learning how to apply statistics to practical problems!\nThis 90-minute tutorial from the author of \u201cBayesian Statistics the Fun Way\u201d will provide a quick overview of the practice of using statistics to solve real world problems. Explanation of statistical topics will follow solving a practical example problem. The focus of the tutorial will be on comparing the performance of two products in an ecommerce catalog. You will learn how to use statistics to:\n\nDetermine the best estimate for the rate the product is purchased.\nImproving this estimate with past performance data.\nCompare the performance of one product to another.\nSee how your estimates change as you get more data.\nModel the impacts that having a sale had on the product\u2019s performance. \nSeparate the effects of the sale from product performance when comparing\n\nAll steps in the tutorial will involve demonstrations with Python code. We\u2019ll be making use of numpy, pandas, matplotlib, jupyter and PyMC3. At the end of this talk you will have walked through the process of reasoning statistically about a real data problem.\n1. Foundations of Probability and Statistics (30 minutes)\nFocus: measuring the performance of a product in an online catalog\nA. Probability the logic of uncertainty (15 minutes)\n\nIntroduction to basics rules probability\nDevelopment of the Binomial Distribution from first principles \n\nB. Statistical Inference: probability in reverse! (15 minutes)\n\nEstimating the probability of an event using the Beta Distribution\nHow our beliefs change over time\nUsing prior information to improve our beliefs.\n\n2. Parameter Estimation and Hypothesis testing (20 minutes)\nFocus: Comparing two products: which is better and by how much?\nA. Hypothesis test as parameter estimate\n\nEstimating two product purchase rates\nComparing these estimates\n\nB. Improving our hypothesis tests with prior probabilities\n\nIncorporating prior probabilities into our estimates\nHow priors protect us from the \u201cearly stopping\u201d problem\n\n3. Linear models for statistical inference (40 minutes)\nFocus: What do we do when our test is influenced by random discounts?\nA. Brief intro to PyMC3\n\nCreating a simple PyMC3 model\n\nB. Rebuilding our problem as a linear model\n\nBasic linear model for product performance\nComparing results\n\nC. Testing more complex situations\n\nadjusting for random discounting of products\nunderstanding the product comparison\nunderstanding impact of the discount", "summary": "This tutorial will offer a quick overview of many of the essentials of statistics used to solve real world problems. We'll start by looking at how to build a simple hypothesis test based on a practical e-commerce problem. Then we'll see how we can expand on this simple test using one of the most powerful tools in statistics: the linear model. No previous experience with statistics required!", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/23/an-introduction-to-probability-and-statistics/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T09:00:00Z", "end": "2019-11-06T10:30:00Z", "duration": "1:30:00"}, {"name": "Introduction to NLP", "performer": ["Mariel Frank"], "@type": "tutorial", "description": "This tutorial is intended for NLP novices who want to gain a high-level understanding of the field and whet their appetites with a bit of code. No prior knowledge of NLP is necessary. Familiarity with basic Python syntax and prior installation of the nltk library is recommended.\nThis tutorial will provide an overview of what NLP is, why it's useful, as well as several major NLP libraries and topics. You'll also have the opportunity to get hands-on with some basic NLP methods using Python.\nYou can access files, etc. at this GitHub repo: https://github.com/marielfrank/pydatanyc_2019", "summary": "Curious about Natural Language Processing (NLP) but don't know where to start? This tutorial is for you! This tutorial will provide an overview of what NLP is, why it's useful, as well as several major NLP libraries and topics. You'll also have the opportunity to get hands-on with some basic NLP methods using Python.", "level": "Novice", "room": "Radio City (6604)", "url": "https://pydata.org/nyc2019/schedule/presentation/70/introduction-to-nlp/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T09:00:00Z", "end": "2019-11-06T10:30:00Z", "duration": "1:30:00"}, {"name": "Visualizing the 2019 Measles Outbreak in NYC (with Python)", "performer": ["Carlos Afonso"], "@type": "tutorial", "description": "1. Introduction\n1.1. Motivation\n\nThe large majority of NYC measles cases are in my neighborhood (Williamsburg, Brooklyn).\nOpportunity to learn/practice fundamental as well as advanced data visualization skills.\nExample of a small data project that can help people understand an important issue.\n\n1.2. What is measles?\n\nMeasles is a highly contagious infectious disease that can cause serious health complications.\nTwo doses of MMR vaccine provide the best protection against measles.\n\n1.3. A brief history of measles in the US\n\nMeasles was declared eliminated from the US in 2000, thanks to an effective vaccination program.\nThe US is amid its largest measles outbreak since 1992, with 1,250 (preliminarily) confirmed cases as of Oct 3, 2019 [CDC]. Most of those cases (649) were in NYC, where the outbreak was declared over on Sep 3, 2019 [NYC Health].\n\n2. Data\n2.1. Data Sources\n\nOf all the affected areas, NYC provides the best data about the 2019 measles outbreak.\nNYC provides raw data about the number of measles cases by date, age, vaccination status, and neighborhood on its NYC Health Measles webpage.\n\n2.2. Data Collection\n\nThe data is collected/updated manually and stored in CSV and XLSX files because the data is relatively small and updated infrequently (only about once a week).\n\n3. Visualizations\nAll data visualizations are shown in the project homepage: https://carlos-afonso.github.io/measles\n3.1. NYC new measles cases by month\n\nExample of how to create a vertical bar chart to display temporal data.\nShow how to adjust the bar chart properties to provide context and clarity.\nFor context: use title and annotations to provide the necessary information.\nFor clarity: remove unnecessary chart elements, format month names, show labels with the number of cases.\nInsights: The bar chart clearly shows that, after peaking in Apr 2019, the number of new measles cases declined progressively until it reached 0 in Aug 2019. This is an indication that the additional MMR vaccination efforts that the NYC Health department started taking in April 2019 seem to have helped control the outbreak.\n\n3.2. NYC measles cases by neighborhood\n\nExample of how to use bokeh to create a bubble map visualization.\nShow and discuss the several design decisions to provide context and clarity.\nExplain how in this case it is better to use a static rather than an interactive map.\nExplain the decision to show labels with the names of the neighborhoods and the respective number of measles. Although the labels \u201cclutter\u201d the map, they are important because they help identify the neighborhoods.\nInsights: The bubble map clearly shows all the NYC neighborhoods with measles cases, using the bubble size to represent the number of cases.\n\n3.3. NYC measles cases by age\n\nExample of how to create a horizontal bar chart to display categorical data.\nExample of a case when it is better to use a horizontal rather than a vertical bar chart.\nShow how to adjust the bar chart properties to provide context and clarity.\nFor context: use title and annotation to provide the necessary information.\nFor clarity: remove unnecessary chart elements, show labels with the number and percentages of cases.\nInsights: The bar chart shows that most of the NYC measles cases are in young children.\n\n3.4. NYC measles cases by vaccination status\n\nTechnically this is a horizontal bar chart similar to the one in the previous section (3.3).\nInsights: This bar chart clearly shows that the large majority of the people who got measles were unvaccinated.", "summary": "The US is amid its largest measles outbreak since 1992, with 1,250 cases as of Oct 3, 2019. Most cases (649) were in NYC, where the outbreak was declared over on Sep 3, 2019. This tutorial creates data visualizations to help understand the measles outbreak in NYC. Bubble maps and bar charts are created using Python (bokeh, matplotlib) and following principles of clarity and context.", "level": "Intermediate", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/33/visualizing-the-2019-measles-outbreak-in-nyc-with-python/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T09:00:00Z", "end": "2019-11-06T10:30:00Z", "duration": "1:30:00"}, {"name": "HoloViz and Matplotlib sprint", "performer": ["Julia Signell", " Hannah Aizenman", " Thomas Caswell", " Thomas Caswell"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/94/holoviz-sprint/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T09:00:00Z", "end": "2019-11-06T10:30:00Z", "duration": "1:30:00"}, {"name": "Jupyter sprint", "performer": ["Saul Shanabrook", " Jason Grout"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/89/jupyter-sprint3/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T09:00:00Z", "end": "2019-11-06T10:30:00Z", "duration": "1:30:00"}, {"name": "Advanced Software Testing for Data Scientists", "performer": ["Raoul-Gabriel Urma"], "@type": "tutorial", "description": "It's fun to develop a model in a Python notebook! But engineering team are always complaining about code maintenance and code quality, asking for production ready code. What can you as a data scientist learn from the software development world to help with this? In this tutorial, you will learn about state of the art testing approach. You will learn how to break down a model implemented in a notebook into separate parts which you can unit test and ensure quality with common tools available in Python. In addition, you will learn how to apply property based testing and test doubles.\nYou will learn about:\n\nHow to structure your preprocessing and modelling code to be testable\nHow to write maintainable tests using unittest and pytest\nHow to use data science validation tools and diagnostics from Pandas and Scikit-learn\nHow to use test doubles to write better tests using unittest.mock\nHow to automatically generate test cases using property-based testing and Hypothesis\n\nSetup\nGit repo to clone\nhttps://github.com/cambridgespark/pydata-testing-for-data-science\nRequirements\nscikit-learn\npandas\nnumpy\npytest\npytruth\npytest-cov\nmarbles\nhypothesis[pandas]\nrequests", "summary": "The journey to deploy a model to production starts with testing it rigorously, including its code implementation. In this tutorial, you will learn about state of the art software testing approach. You will learn how to write unit tests with enhanced diagnostics, leverage validation tools from numpy, pandas, scikit-learn, apply test doubles and generate test cases using property-based testing.", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/6/advanced-software-testing-for-data-scientists/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:45:00Z", "end": "2019-11-06T12:15:00Z", "duration": "1:30:00"}, {"name": "How to Prove You\u2019re Right: A/B Testing with SciPy", "performer": ["Hillary Green-Lerman", " Michoel Snow"], "@type": "tutorial", "description": "Prerequisites\nStudents should be familiar with basic Python, Jupyter notebooks, and loading and selecting data using Pandas.\nPart 1: Understanding the need for statistical testing\n\n\nLearners will be able to define an \"A/B Test\" and give examples of how they are used by companies to aide in data-driven decision-making.\n\n\nLearner will be able to distinguish between categorical and continuous measurements\n\n\nLearner will be able to use SciPy to generate normal (for continuous measurements) and binomial (for categorical measurements) distributions\n\n\nLearner will be able to simulate different A/B Tests and understand how sample size and effect size can cause misleading A/B test results\n\n\nPart 2: Performing Statistical Tests with SciPy\n\n\nLearner will be able to explain how hypothesis testing can be used to validate A/B testing results\n\n\nLearner will able to define \"null hypothesis\" and explain it's importance for hypothesis testing\n\n\nLearner will be able to estimate a sample size for an A/B Test using simulations\n\n\nLearner will be able to select one of the following hypothesis tests based on the type of metric they are reporting: Chi Squared, Student's T Test, or ANOVA\n\n\nLearner will be able to estimate the necessary sample size for an A/B test using simulations\n\n\nPart 3: Short Cuts\n\n\nLearner will be able to use online tools for estimating sample size\n\n\nLearner will be able to use online tools performing hypothesis testing", "summary": "So you want to run an A/B test!  How long should you run it for? How will you know if an observed difference is real? Does it matter what my A/B test is measuring? All of these questions and more can be answered by learning about Hypothesis Testing in SciPy.  You might have heard of T-Tests, Chi Squared, and ANOVA in your high school stats class, but you've never heard them explained like this!", "level": "Novice", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/19/how-to-prove-youre-right-ab-testing-with-scipy/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:45:00Z", "end": "2019-11-06T12:15:00Z", "duration": "1:30:00"}, {"name": "Introduction to Language Modeling", "performer": ["Aditi Khullar", " Eugene Tang"], "@type": "tutorial", "description": "In this 90-minute intermediate-level tutorial at PyData NYC we will introduce the concept of language modeling. In the first half we will explain language models, how they are used and why they are important in the domain of Natural Language Processing. In the second half we will be taking the attendees through a hands-on workshop where they will develop a language model based on a sample dataset and talk through some of the results and gotchas of working with language models.", "summary": "In this two part tutorial we will start with introduction of language models, how they are used and why they are important in the domain of Natural Language Processing. In the second half we will be taking the attendees through a hands-on workshop where they will develop a language model based on a sample dataset and talk through some of the results and gotchas of working with language models.", "level": "Intermediate", "room": "Radio City (6604)", "url": "https://pydata.org/nyc2019/schedule/presentation/60/introduction-to-language-modeling/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:45:00Z", "end": "2019-11-06T12:15:00Z", "duration": "1:30:00"}, {"name": "Swiftly turn Jupyter notebooks into pretty web apps", "performer": ["Michal Mucha"], "@type": "tutorial", "description": "Learn to swiftly turn your Jupyter notebooks into pretty web apps, tailored to serve the needs of your colleagues and users. \nThings that look pretty and are simple to use end up being used more - use this principle to share the meaningful work you already are doing with a bigger audience.  \nCreate a beautiful \"Reception Lobby\" for your work, without spending an eternity building custom front ends. Make your exploration and modeling instantly shareable and alive for any audience in your organization, leveraging the power of open source projects such as ipywidgets, Voila, traitlets, pygal, vuetify, and more.\nThe ability to build data science products quickly will give you new opportunities to collaborate with various audiences in your organization and contribute to the increase in the overall data literacy of everyone involved.\nPlease visit this GitHub repository for quick preparation steps and a short brain teaser. I will post notebooks to this repository in the days prior to the workshop.\nAgenda\nThis tutorial will begin with a quick summary of ways to serve interactive Python-backed pages, and revisit key points for understanding your audience and sharing outcomes successfully.\nAfter this brief intro, we will move on to a hands-on session, where we begin with a notebook with financial cash flow analysis and end up with a Voila-powered interactive simulation of outcomes dependent on multiple variables.\nAt the end, we will look at some examples and sources of reusable code to inspire your projects and give you great tools to start with.\nWhat you will bring home\n\nhands on experience of turning a jupyter notebook into an independent app  \nan interactive investment cash flow simulator\nbits and pieces that will enable you to quickly assemble exciting apps of your own", "summary": "Communicating outcomes is a core part of being a great data scientist. As time consuming as it can get, designing and creating presentations that are beautiful and engaging is well worth the effort - it makes the outcomes of your work more accessible, interpretable and reusable.\nThis workshop teaches new open source tools (Voila) to build and serve pretty interactive web apps leveraging Jupyter.", "level": "Intermediate", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/58/swiftly-turn-jupyter-notebooks-into-pretty-web-apps/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:45:00Z", "end": "2019-11-06T12:15:00Z", "duration": "1:30:00"}, {"name": "HoloViz and Matplotlib sprint", "performer": ["Julia Signell", " Hannah Aizenman", " Thomas Caswell", " Thomas Caswell"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/95/holoviz-sprint2/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:45:00Z", "end": "2019-11-06T12:15:00Z", "duration": "1:30:00"}, {"name": "Jupyter sprint", "performer": ["Saul Shanabrook", " Jason Grout"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/90/jupyter-sprint4/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:45:00Z", "end": "2019-11-06T12:15:00Z", "duration": "1:30:00"}, {"name": "Machine learning from scratch using the scientific Python stack", "performer": ["Lara Kattan"], "@type": "tutorial", "description": "Level-up your data science by diving deep into the innards of scientific Python\nBuilding your first few scikit-learn models is gratifying, but where do you go from there? Gaining a deeper understanding of the numerical methods underlying your favorite modeling library is important for advancing in your data science career as it allows you to make more informed decisions about efficiency and run-time. Dive deep into the innards of the scientific Python stack (SciPy and NumPy) in a way relevant for data science, statistics and related numerical fields. \nTutorial structure: review math, write two algorithms from scratch\nThis tutorial will be have two parts: we'll spend 45 minutes reviewing numerical solution methods by hand, then dedicate 45 minutes to re-writing a popular machine learning algorithm from scratch using only NumPy and SciPy. In particular, we'll explore matrix decompositions for feature extraction and NLP, including topic modeling, plus gradient descent and the Fast Fourier Transform (FFT). We'll end by using NumPy and SciPy to code up PCA/LSA and gradient descent by hand! This should give you the confidence to dive deeper into the code base for Python machine learning libraries like SKLearn and give you the knowledge to start contributing to the development of machine learning open source Python software. \nAfter this tutorial, you will:\n\nUnderstand the data structures at the heart of the Python scientific stack (NumPy and SciPy), including sparse matrices\nGain a thorough review of the numerical solution methods of 1) matrix decomposition, 2) stochastic gradient descent, and 3) the Fast Fourier Transform (FFT)\nKnow how matrix decomposition and the FFT are applied in data science and related fields\nWrite your own PCA (principal components analysis) and stochastic gradient descent algorithms from scratch in Python, using only SciPy and NumPy\nDeepen your appreciation for the math and numerical solution methods underlying many of the most common and popular machine learning models  \nBe better prepared to dive into the SKLearn code base in order to begin contributing to open source machine learning software in Python", "summary": "You know that, under the hood, your favorite Python modelling libraries are using numerical methods for optimization. Now increase the efficiency of your models by deepening your understanding of scientific Python (SciPy and NumPy). We'll review sparse matrices, matrix decomposition, gradient descent & the Fourier Transform, plus write an algorithm from scratch using only NumPy/SciPy!", "level": "Intermediate", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/12/machine-learning-from-scratch-using-the-scientific-python-stack/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T13:15:00Z", "end": "2019-11-06T14:45:00Z", "duration": "1:30:00"}, {"name": "New Trends in Estimation and Inference", "performer": ["Cameron Davidson-Pilon"], "@type": "tutorial", "description": "I'll present to you modern solutions to old problems in statistical inference and estimation. This tutorial will introduce new ideas in optimization and bridge the gap between statistical models and your business' problems and scientific problems. What I expect you to learn from this tutorial is:\n\nhow to code maximum likelihood estimation problems in Python\nwriting (much) better code for numerical optimizations using automatic differentiation\nhow to transform abstract model parameters into business logic, using the delta method\navoiding null hypothesis testing and instead focusing on modern interpretations of statistical inference\n\nThis will be an optionally hands-on tutorial, so either bring a laptop or sit back and treat it like a lecture!", "summary": "I'll present to you modern solutions to old problems in statistical inference and estimation. This tutorial will introduce new ideas in optimization and bridge the gap between statistical models and your business' problems / scientific problems. This will be an optionally hands-on tutorial, so either bring a laptop or sit back and treat it like a lecture!", "level": "Experienced", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/18/new-trends-in-estimation-and-inference/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T13:15:00Z", "end": "2019-11-06T14:45:00Z", "duration": "1:30:00"}, {"name": "Neural Networks for Natural Language Processing", "performer": ["Matti Lyra"], "@type": "tutorial", "description": "Neural Networks for NLP\nDeep, pretrained neural networks have in the past roughly 2 years become a staple in the NLP community. The driver for this development has been the success of language modelling as a pretraining task (transfer learning) as well as new and improved network architectures and training methods. State-of-art models like ULMfit, BERT, GPT-2, XLM or AWD-LSTM all use some kind of a language modelling task to first train a network, which can then be used to perform other downstream tasks.\nTutorial Overview\nIn this tutorial I will first cover the basics of neural networks as applied to NLP before moving on to the ideas behind these state-of-art methods. A rough outline of the tutorial is as follows:\n\nNeural network basics (10-15 minutes)\nRecurrent neural networks and language models (10-15 minutes)\nLanguage model pretraining and transfer learning (30 minutes)\nAttention and the Transformer model (30 minutes)\n\nI aim to focus on practical applications (document classification, sequence labelling) with real world examples. At the end of the tutorial you should be able to use some of the state-of-art methods in your own projects and have a better understanding how the building blocks of those methods fit together.\nPrerequisites\nYou'll get most out of the tutorial if you are already familiar with Python. Basic familiarity with neural networks is assumed, it helps if you have at least a passing knowledge of loss functions, gradient descent and the like. Familiarity with specific neural network packages is not required, although familiarity with pytorch doesn't hurt. The tutorial will feature lots of code, but all code is available for offline viewing as notebooks. The code itself is not the main point!\nAll associated code is available on Github from https://github.com/mattilyra/pydatanyc_2019", "summary": "Deep, pretrained neural networks have in the past roughly 2 years become a staple in the NLP community. The driver for this development has been the success of transfer learning that build on large pretrained language models. In this tutorial I will cover the main concepts behind these methods.", "level": "Intermediate", "room": "Radio City (6604)", "url": "https://pydata.org/nyc2019/schedule/presentation/34/neural-networks-for-natural-language-processing/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T13:15:00Z", "end": "2019-11-06T14:45:00Z", "duration": "1:30:00"}, {"name": "From Raw Recruit Scripts to Perfect Python (in 90 minutes)", "performer": ["Stanley van der Merwe", " Petr Wolf"], "@type": "tutorial", "description": "Whether developing new models in Jupyter Notebooks or porting existing code from older infrastructure or other technologies (e.g. Excel, SAS), data scientists are often faced with disorganized structure, reproducibility issues or low run-time performance.\nModel implementation quality and performance plays a critical role in successful deployment, continued use and future maintenance costs. Key drivers of this success include modularized code and tests that are well defined, both of which often get neglected or left out entirely.\nIn this tutorial we will start with a Jupyter Notebook that represents a sample model with typical shortcomings, such as a mixing of input data processing with model logic, missing tests, lack of usage examples or confusing code.\nIn a series of steps, we will incrementally refactor the code into intuitive modular python, using the best tools from the python ecosystem.\nYou will learn to\n\nstructure your code in composable and re-usable blocks with in-line documentation and examples\ncatalog and organize boilerplate data sourcing (using Intake)\nuse automated testing (pytest and hypothesis) and static code analysis (PyLint) to guarantee code quality and reproducibility\nanalyze performance (cProfile, line_profiler) to identify hot-spots and guide run-time optimization\napply just-in-time compilation (JIT) and vectorization using numba for even faster performance\n\nThis tutorial is for you if you \n\nwant to take the next step after beginner python tutorials\nmainly use Jupyter Notebooks for your work and want to add more tools to your toolbox\nwant to help your team in improving code quality\nare in the process of migrating code or models from other technologies (SAS, Excel) and want to use best-practices from the start", "summary": "After mastering python basics, it gets increasingly difficult to produce well-structured and test-driven code. Code that is fit for purpose in the short term could become stale due to a lack of composability and testing. In this tutorial, we will go beyond a typical modeling notebook and turn it into well-structured python code equipped with tests that are easy to understand, maintain and extend.", "level": "Novice", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/14/from-raw-recruit-scripts-to-perfect-python-in-90-minutes/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T13:15:00Z", "end": "2019-11-06T14:45:00Z", "duration": "1:30:00"}, {"name": "Pandas sprint", "performer": ["Jeff Reback"], "@type": "plenary", "description": "Check out the Pandas contributor guide here: https://pandas.pydata.org/pandas-docs/stable/development/contributing.html\nSprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/85/pandas-sprint/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T13:15:00Z", "end": "2019-11-06T14:45:00Z", "duration": "1:30:00"}, {"name": "Jupyter sprint", "performer": ["Saul Shanabrook", " Jason Grout"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/87/jupyter-sprint/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T13:15:00Z", "end": "2019-11-06T14:45:00Z", "duration": "1:30:00"}, {"name": "Hacking the Data Science Challenge", "performer": ["Michoel Snow", " Hillary Green-Lerman"], "@type": "tutorial", "description": "Coding a good data science model for work is not the same as programming a good data science challenge for an interview. This tutorial will cover what you need to know in order to make your challenge stand out from the crowd as well as highlighting common mistakes and pitfalls to avoid.  We will be stressing best practices more than specific machine learning techniques.  This tutorial assumes you have a working knowledge of Pandas and at least one plotting library.\nDuring the tutorial you will be split up into groups to work through different parts of the data science takehome challenge.  The code for the tutorial can be found at https://github.com/MichoelSnow/pydata_nyc_2019\nThe focus for this tutorial will be following topics: \nWorking with trick data\n\nInvestigating a data set for natural and synthetic errors\nHow to handle errors in the data\n\nOutlining your Challenge\n\nWhat time requirements actually mean\nHow much time you should spend on a challenge \nHow to infer what you will be graded on\nHow to properly allocate your time\n\nExploratory Data Analysis (EDA)\n\nWhat makes a good plot\nChoosing what to plot and how to plot it\nKnowing when you are done with EDA\n\nModeling data\n\nHow to choose the appropriate model for the company\nHow to explain model findings\n\nIf time permits we will also be discussing the following general best practices:\n\nHow to make sure your code runs for your interviewer\nHow to document and comment appropriately", "summary": "You\u2019ve passed the first round of interviews and are now given a data science take home challenge.  How can you analyze the data, demonstrate your data science abilities, and tick the required checkboxes, all within the allotted time? This tutorial will take you through the process of working through a data science challenge using pre-built functions to automate the boring stuff.", "level": "Novice", "room": "Winter Garden (5412)", "url": "https://pydata.org/nyc2019/schedule/presentation/9/hacking-the-data-science-challenge/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T15:00:00Z", "end": "2019-11-06T16:30:00Z", "duration": "1:30:00"}, {"name": "Bayesian Inference for Fun and Profit", "performer": ["Mitzi Morris"], "@type": "tutorial", "description": "Takeaways from this talk:\n\nUnderstanding of the mechanics of Bayesian Inference\nmodel specification - how to describe the data generating process\nmodel checking - determining how well the model fits the data\n\nposterior predictive checks - do the model predictions make sense?\n\n\nIntroduction to estimation by Markov-Chain Monte Carlo (MCMC) simulation,\nusing Stan (mc-stan.org), a probabilistic programming platform which does\nfull Bayesian inference using Hamiltonian Monte Carlo (HMC).  \n\n\nAbility to discriminate between Bayesian and point-wise estimation methods,\nregardless of what they are called.", "summary": "In this talk I will show you how to do Bayesian Inference \nvia fully-worked examples using game data from baseball and soccer.\nBy comparing predictions made by plugging in Bayesian estimates\nto those made by plugging in maximum likelihood estimates,\nwe see how critical it is that your predictions\nreflect not just your estimate of the most likely outcome,\nbut also your degree of uncertainty.", "level": "Intermediate", "room": "Music Box (5411)", "url": "https://pydata.org/nyc2019/schedule/presentation/25/bayesian-inference-for-fun-and-profit/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T15:00:00Z", "end": "2019-11-06T16:30:00Z", "duration": "1:30:00"}, {"name": "Role playing Annotation workshop", "performer": ["Agata Sumowska", " Bhargav Srinivasa Desikan", " Laurence Warner", " Lev Konstantinovskiy"], "@type": "tutorial", "description": "You will attempt to solve the business problem of analysing a homepage of a small business in order to classify it into a business category (bakery, cafe, deli etc ) and extract the business name and address.\nThe NLP techniques used will be text classification and information extraction, however we will not run any code during the workshop.\nYou will experience two modes of team communication and two approaches to modelling. You will try yourself in different team roles and learn the best practices of information flow, project scoping and NLP model building.\nPlease bring a laptop.\nIf you have your lucky dice, bring them too, we will have some spare.", "summary": "This workshop is about three things : team communication, project scoping and approaches to modelling. It is hands on, but doesn't involve coding. You will experience being part of an NLP project. In a team of 3 you will be assigned roles of an NLP engineer, an annotator or a business stakeholder. Your team will have a Game Master who will personally guide you through the journey, similar to D&D.", "level": "Novice", "room": "Radio City (6604)", "url": "https://pydata.org/nyc2019/schedule/presentation/21/role-playing-annotation-workshop/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T15:00:00Z", "end": "2019-11-06T16:30:00Z", "duration": "1:30:00"}, {"name": "A Primer on Gaussian Processes for Regression Analysis", "performer": ["Chris Fonnesbeck"], "@type": "tutorial", "description": "Nowadays, there are many ways of building data science models using Python, including statistical and machine learning methods. I will introduce probabilistic models, which use Bayesian statistical methods to quantify all aspects of uncertainty relevant to your problem, and provide inferences in simple, interpretable terms using probabilities. A particularly flexible class of probabilistic models uses Bayesian non-parametric methods, which allow models to vary in complexity depending on how much data are available. In doing so, they avoid the over-fitting that is common in machine learning and statistical modeling. I will demonstrate the basics of Bayesian non-parametric modeling in Python. Specifically, I will introduce Gaussian processes (GP), and show how they can be applied to regression analysis using a few examples.\nOutline\n\nRegression models: the basics (10 min)\nWhat\u2019s the usual approach to regression, and what are its limitations?\n\n\nIntroduction to probabilistic modeling (15 min)\nHow can you model complex things using a Gaussian (normal) distributions?\n\n\nWhat is a Gaussian process? (20 min)\nAn overview of the features and properties of Gaussian processes.\n\n\nBuilding Gaussian process models (20 min)\nSelecting your covariance function to suit your problem.\n\n\nFitting Gaussian process models (15 min)\nHow you fit your GP depends on what you need it to do, and how much data you have.\n\n\nModel checking and prediction (10 min)\nDoes my GP work as advertised? What can I do with it?", "summary": "Gaussian processes are flexible probabilistic models that can be used to perform Bayesian regression analysis without having to provide pre-specified functional relationships between the variables. This tutorial will introduce new users to specifying, fitting and validating Gaussian process models in Python.", "level": "Intermediate", "room": "Broadway (5202)", "url": "https://pydata.org/nyc2019/schedule/presentation/31/a-primer-on-gaussian-processes-for-regression-analysis/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T15:00:00Z", "end": "2019-11-06T16:30:00Z", "duration": "1:30:00"}, {"name": "Pandas sprint", "performer": ["Jeff Reback"], "@type": "plenary", "description": "Check out the Pandas contributor guide here: https://pandas.pydata.org/pandas-docs/stable/development/contributing.html\nSprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Ambassador (6202)", "url": "https://pydata.org/nyc2019/schedule/presentation/86/pandas-sprint2/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T15:00:00Z", "end": "2019-11-06T16:30:00Z", "duration": "1:30:00"}, {"name": "Jupyter sprint", "performer": ["Saul Shanabrook", " Jason Grout"], "@type": "plenary", "description": "Sprints are an excellent opportunity to make your first contribution to an open source project in a friendly, guided environment. They are also an excellent opportunity to meet the maintainers of the tools you use.\nYou do not need specific experience or context before coming to a sprint. Projects will have a list of bite-sized issues looking for someone (you!) to fix them! Just bring your laptop!", "summary": "A sprint is an open session where attendees can work on a specific open source project with the guidance of a core contributor. Walk-ins after the session has started are welcome!", "level": "Novice", "room": "Belasco (6203)", "url": "https://pydata.org/nyc2019/schedule/presentation/88/jupyter-sprint2/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T15:00:00Z", "end": "2019-11-06T16:30:00Z", "duration": "1:30:00"}, {"name": "Breakfast & Registration", "performer": null, "@type": "other", "description": null, "summary": null, "level": null, "room": null, "url": "https://pydata.orghttps://pydata.org/nyc2019/schedule/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T08:00:00Z", "end": "2019-11-06T09:00:00Z", "duration": "1:00:00"}, {"name": "Coffee Break", "performer": null, "@type": "other", "description": null, "summary": null, "level": null, "room": null, "url": "https://pydata.orghttps://pydata.org/nyc2019/schedule/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T10:30:00Z", "end": "2019-11-06T10:45:00Z", "duration": "0:15:00"}, {"name": "Lunch", "performer": null, "@type": "other", "description": null, "summary": null, "level": null, "room": null, "url": "https://pydata.orghttps://pydata.org/nyc2019/schedule/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T12:15:00Z", "end": "2019-11-06T13:15:00Z", "duration": "1:00:00"}, {"name": "Break", "performer": null, "@type": "other", "description": null, "summary": null, "level": null, "room": null, "url": "https://pydata.orghttps://pydata.org/nyc2019/schedule/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T14:45:00Z", "end": "2019-11-06T15:00:00Z", "duration": "0:15:00"}, {"name": "Wrap Up", "performer": null, "@type": "other", "description": null, "summary": null, "level": null, "room": null, "url": "https://pydata.orghttps://pydata.org/nyc2019/schedule/", "date": "Wednesday Nov. 6, 2019", "start": "2019-11-06T16:30:00Z", "end": "2019-11-06T17:00:00Z", "duration": "0:30:00"}]}